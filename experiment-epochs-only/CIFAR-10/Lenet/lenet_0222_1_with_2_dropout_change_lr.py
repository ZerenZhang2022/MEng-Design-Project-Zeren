# -*- coding: utf-8 -*-
"""Lenet-0222-1-with 2 dropout-change lr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rtL3dLz7Wk37Bx48kqj1qLk5T6XxD3al
"""

import os
from google.colab import drive
drive.mount('/content/drive')

# Keras
import keras
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout
from keras.utils.np_utils import to_categorical
from keras import optimizers
import tensorflow as tf
from keras.datasets import cifar10

# NumPy
import numpy as np

# Python Std Lib
import os

# User Lib
dropout = 0.2

# get the training and test data
(input_train, output_train), (input_test, output_test) = cifar10.load_data()

# creating the basic model
model = Sequential()

# 30 Conv Layer
model.add(Conv2D(30, kernel_size=(3, 3), padding='valid', activation='relu', input_shape=(32, 32, 3)))
# 15 Max Pool Layer
model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))
model.add(Dropout(dropout))
# 13 Conv Layer
model.add(Conv2D(13, kernel_size=(3,3), padding='valid', activation='relu'))
# 6 Max Pool Layer
model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))
model.add(Dropout(dropout))
# Flatten the Layer for transitioning to the Fully Connected Layers
model.add(Flatten())
# 120 Fully Connected Layer
model.add(Dense(120, activation='relu'))
# 84 Fully Connected Layer
model.add(Dense(86, activation='relu'))
# 10 Output
model.add(Dense(10, activation='softmax'))

# compile the model
initial_learning_rate = 0.001
sgd = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)

def scheduler(epoch, lr):
  if epoch < 100:
    return 0.02
  elif epoch < 200:
    return 0.01
  elif epoch < 400:
    return 0.005
  elif epoch < 700:
    return 0.003
  else: 
    return 0.001    

callback = tf.keras.callbacks.LearningRateScheduler(scheduler)



model.compile(optimizer=sgd,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
save_dir = '/content/drive/MyDrive/ECE6930/Lenet-0222-01/changelr/2dropout/1'
checkpointer = keras.callbacks.ModelCheckpoint(os.path.join(save_dir, '{epoch:03d}.h5'), monitor='val_loss', verbose=0,						save_best_only=False, 
 								save_weights_only=False, mode='auto', 
 								period=10)


# train the model
history = model.fit(input_train/255, to_categorical(output_train), epochs=1000, 
           validation_data=(input_test/255, to_categorical(output_test)),
                    batch_size=128,callbacks=[callback,checkpointer])


# test
score = model.evaluate(input_test/255, to_categorical(output_test), batch_size=128)

# print test set results
print("Testset Loss: %f" % score[0])
print("Testset Accuracy: %f" % score[1])

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
#plt.plot(history.history['val_loss'])
plt.title("model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("epoch")
plt.legend(["train","test"],loc="lower right")
plt.show()

print(history.history)