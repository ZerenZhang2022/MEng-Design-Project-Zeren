{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxJ8BOvDsrTO",
        "outputId": "8318b3e8-d0ce-4d21-d1d7-799d7755cdaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWkTClo3o42v"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.initializers import RandomNormal  \n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
        "from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n",
        "\n",
        "weight_decay  = 0.0001# 新增\n",
        "batch_size    = 512\n",
        "epochs        = 500\n",
        "#iterations    = 391\n",
        "num_classes   = 10\n",
        "dropout       = 0.2\n",
        "log_filepath  = '/content/drive/MyDrive/ECE6930/NiN-0207-01/new7'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXHG6-sdqA1a"
      },
      "outputs": [],
      "source": [
        "def color_preprocessing(x_train,x_test):\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    mean = [125.307, 122.95, 113.865]\n",
        "    std  = [62.9932, 62.0887, 66.7048]\n",
        "    for i in range(3):\n",
        "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
        "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
        "    return x_train, x_test\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 80:\n",
        "        return 0.001\n",
        "    elif epoch <= 140:\n",
        "        return 0.0005\n",
        "    else: return 0.0001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv_AWVkpqElf"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), \n",
        "                     input_shape=x_train.shape[1:]))# 32, 32, 3\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
        "\n",
        "    model.add(Dropout(dropout))\n",
        "    \n",
        "    model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
        "\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #was sgd\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3F7DtdbqGiP",
        "outputId": "fce9417b-4943-4902-b870-71293b56bf76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes) # one-hot 编码\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes) # one-hot 编码\n",
        "x_train, x_test = color_preprocessing(x_train, x_test) # 把减均值，除以标准差封装成了函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x7ZQK06r9-T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGxuElYaqJz9",
        "outputId": "574c590b-a5d6-484f-ee56-c744704afb21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 192)       14592     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 192)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 160)       30880     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 160)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 96)        15456     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 32, 32, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 96)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 192)       460992    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 192)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 192)       37056     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16, 16, 192)       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 16, 192)       37056     \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16, 16, 192)       0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 192)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 192)         331968    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 192)         37056     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 10)          1930      \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8, 8, 10)          0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 10)               0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 966,986\n",
            "Trainable params: 966,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/500\n",
            "98/98 [==============================] - 45s 323ms/step - loss: 2.2206 - accuracy: 0.2213 - val_loss: 2.0528 - val_accuracy: 0.3159 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "98/98 [==============================] - 31s 310ms/step - loss: 2.0106 - accuracy: 0.3229 - val_loss: 1.8676 - val_accuracy: 0.3692 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 1.7386 - accuracy: 0.4108 - val_loss: 1.6983 - val_accuracy: 0.4362 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 1.6114 - accuracy: 0.4708 - val_loss: 1.5497 - val_accuracy: 0.4928 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 1.5086 - accuracy: 0.5119 - val_loss: 1.4314 - val_accuracy: 0.5437 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 1.4430 - accuracy: 0.5392 - val_loss: 1.3775 - val_accuracy: 0.5629 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 1.3788 - accuracy: 0.5670 - val_loss: 1.3765 - val_accuracy: 0.5759 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 1.3266 - accuracy: 0.5878 - val_loss: 1.3117 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 1.2911 - accuracy: 0.6017 - val_loss: 1.3141 - val_accuracy: 0.5968 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "98/98 [==============================] - 33s 333ms/step - loss: 1.2491 - accuracy: 0.6191 - val_loss: 1.3027 - val_accuracy: 0.5979 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 1.2323 - accuracy: 0.6235 - val_loss: 1.2160 - val_accuracy: 0.6291 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 1.2043 - accuracy: 0.6355 - val_loss: 1.1953 - val_accuracy: 0.6437 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 1.1690 - accuracy: 0.6474 - val_loss: 1.2533 - val_accuracy: 0.6284 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 1.1399 - accuracy: 0.6588 - val_loss: 1.1489 - val_accuracy: 0.6563 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 1.1146 - accuracy: 0.6666 - val_loss: 1.1919 - val_accuracy: 0.6563 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "98/98 [==============================] - 32s 322ms/step - loss: 1.1065 - accuracy: 0.6717 - val_loss: 1.2071 - val_accuracy: 0.6596 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 1.0820 - accuracy: 0.6807 - val_loss: 1.0789 - val_accuracy: 0.6859 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "98/98 [==============================] - 32s 323ms/step - loss: 1.0639 - accuracy: 0.6872 - val_loss: 1.1097 - val_accuracy: 0.6752 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 1.0460 - accuracy: 0.6945 - val_loss: 1.2455 - val_accuracy: 0.6686 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 1.0298 - accuracy: 0.6993 - val_loss: 1.1027 - val_accuracy: 0.6901 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 1.0232 - accuracy: 0.7025 - val_loss: 1.1288 - val_accuracy: 0.6808 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "98/98 [==============================] - 31s 321ms/step - loss: 1.0050 - accuracy: 0.7092 - val_loss: 1.0305 - val_accuracy: 0.7033 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "98/98 [==============================] - 33s 332ms/step - loss: 0.9845 - accuracy: 0.7164 - val_loss: 1.1201 - val_accuracy: 0.6878 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.9655 - accuracy: 0.7234 - val_loss: 1.0082 - val_accuracy: 0.7183 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "98/98 [==============================] - 31s 319ms/step - loss: 0.9528 - accuracy: 0.7262 - val_loss: 1.0464 - val_accuracy: 0.7078 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "98/98 [==============================] - 32s 325ms/step - loss: 0.9425 - accuracy: 0.7316 - val_loss: 1.0193 - val_accuracy: 0.7168 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.9419 - accuracy: 0.7303 - val_loss: 1.0064 - val_accuracy: 0.7248 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.9343 - accuracy: 0.7357 - val_loss: 1.0754 - val_accuracy: 0.7108 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "98/98 [==============================] - 35s 360ms/step - loss: 0.9102 - accuracy: 0.7422 - val_loss: 1.0513 - val_accuracy: 0.7105 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "98/98 [==============================] - 33s 332ms/step - loss: 0.9056 - accuracy: 0.7434 - val_loss: 1.0100 - val_accuracy: 0.7161 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 0.8953 - accuracy: 0.7485 - val_loss: 1.0367 - val_accuracy: 0.7281 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "98/98 [==============================] - 32s 323ms/step - loss: 0.8952 - accuracy: 0.7478 - val_loss: 1.0221 - val_accuracy: 0.7213 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "98/98 [==============================] - 32s 322ms/step - loss: 0.8777 - accuracy: 0.7530 - val_loss: 1.0026 - val_accuracy: 0.7350 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.8790 - accuracy: 0.7542 - val_loss: 0.9408 - val_accuracy: 0.7368 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 0.8702 - accuracy: 0.7569 - val_loss: 0.9676 - val_accuracy: 0.7336 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.8686 - accuracy: 0.7571 - val_loss: 0.9356 - val_accuracy: 0.7424 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "98/98 [==============================] - 33s 331ms/step - loss: 0.8577 - accuracy: 0.7625 - val_loss: 0.9449 - val_accuracy: 0.7419 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "98/98 [==============================] - 32s 321ms/step - loss: 0.8566 - accuracy: 0.7617 - val_loss: 0.9600 - val_accuracy: 0.7432 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "98/98 [==============================] - 32s 321ms/step - loss: 0.8418 - accuracy: 0.7679 - val_loss: 0.9514 - val_accuracy: 0.7458 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "98/98 [==============================] - 32s 322ms/step - loss: 0.8353 - accuracy: 0.7700 - val_loss: 0.8972 - val_accuracy: 0.7603 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.8335 - accuracy: 0.7700 - val_loss: 1.0486 - val_accuracy: 0.7382 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "98/98 [==============================] - 31s 320ms/step - loss: 0.8286 - accuracy: 0.7715 - val_loss: 0.9342 - val_accuracy: 0.7436 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.8223 - accuracy: 0.7743 - val_loss: 0.9199 - val_accuracy: 0.7517 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "98/98 [==============================] - 33s 337ms/step - loss: 0.8187 - accuracy: 0.7758 - val_loss: 0.9287 - val_accuracy: 0.7526 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "98/98 [==============================] - 32s 321ms/step - loss: 0.8147 - accuracy: 0.7782 - val_loss: 1.1833 - val_accuracy: 0.7119 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.8208 - accuracy: 0.7749 - val_loss: 1.0264 - val_accuracy: 0.7299 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.7969 - accuracy: 0.7826 - val_loss: 0.9741 - val_accuracy: 0.7517 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "98/98 [==============================] - 32s 321ms/step - loss: 0.7947 - accuracy: 0.7836 - val_loss: 0.9074 - val_accuracy: 0.7539 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.7943 - accuracy: 0.7865 - val_loss: 0.9265 - val_accuracy: 0.7517 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "98/98 [==============================] - 32s 323ms/step - loss: 0.8031 - accuracy: 0.7811 - val_loss: 0.9082 - val_accuracy: 0.7551 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "98/98 [==============================] - 31s 318ms/step - loss: 0.7926 - accuracy: 0.7846 - val_loss: 0.9450 - val_accuracy: 0.7613 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.7808 - accuracy: 0.7899 - val_loss: 0.9155 - val_accuracy: 0.7518 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "98/98 [==============================] - 30s 311ms/step - loss: 0.7843 - accuracy: 0.7876 - val_loss: 0.8900 - val_accuracy: 0.7580 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.7785 - accuracy: 0.7905 - val_loss: 0.9097 - val_accuracy: 0.7657 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.7668 - accuracy: 0.7945 - val_loss: 0.8716 - val_accuracy: 0.7717 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.7742 - accuracy: 0.7924 - val_loss: 0.9585 - val_accuracy: 0.7555 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.7726 - accuracy: 0.7924 - val_loss: 0.9551 - val_accuracy: 0.7565 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "98/98 [==============================] - 33s 334ms/step - loss: 0.7609 - accuracy: 0.7961 - val_loss: 0.8716 - val_accuracy: 0.7669 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.7629 - accuracy: 0.7960 - val_loss: 0.9052 - val_accuracy: 0.7605 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "98/98 [==============================] - 32s 322ms/step - loss: 0.7572 - accuracy: 0.7962 - val_loss: 0.9095 - val_accuracy: 0.7627 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.7503 - accuracy: 0.7995 - val_loss: 0.9121 - val_accuracy: 0.7647 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.7571 - accuracy: 0.7970 - val_loss: 0.9693 - val_accuracy: 0.7560 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.7458 - accuracy: 0.8014 - val_loss: 0.9874 - val_accuracy: 0.7587 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "98/98 [==============================] - 33s 337ms/step - loss: 0.7450 - accuracy: 0.8032 - val_loss: 0.9164 - val_accuracy: 0.7702 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "98/98 [==============================] - 31s 318ms/step - loss: 0.7422 - accuracy: 0.8037 - val_loss: 0.8965 - val_accuracy: 0.7692 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.7342 - accuracy: 0.8041 - val_loss: 0.8879 - val_accuracy: 0.7673 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.7362 - accuracy: 0.8048 - val_loss: 0.9197 - val_accuracy: 0.7728 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.7386 - accuracy: 0.8040 - val_loss: 0.8826 - val_accuracy: 0.7671 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.7344 - accuracy: 0.8054 - val_loss: 0.9261 - val_accuracy: 0.7548 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.7329 - accuracy: 0.8064 - val_loss: 0.8914 - val_accuracy: 0.7713 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.7295 - accuracy: 0.8088 - val_loss: 0.9546 - val_accuracy: 0.7701 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.7265 - accuracy: 0.8077 - val_loss: 0.9308 - val_accuracy: 0.7693 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.7218 - accuracy: 0.8106 - val_loss: 0.9141 - val_accuracy: 0.7639 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.7277 - accuracy: 0.8087 - val_loss: 0.9267 - val_accuracy: 0.7637 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "98/98 [==============================] - 31s 320ms/step - loss: 0.7245 - accuracy: 0.8114 - val_loss: 0.9195 - val_accuracy: 0.7685 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.7229 - accuracy: 0.8113 - val_loss: 0.9238 - val_accuracy: 0.7696 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.7162 - accuracy: 0.8130 - val_loss: 0.9442 - val_accuracy: 0.7691 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.7186 - accuracy: 0.8116 - val_loss: 0.9428 - val_accuracy: 0.7651 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.7248 - accuracy: 0.8103 - val_loss: 0.8930 - val_accuracy: 0.7724 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "98/98 [==============================] - 31s 310ms/step - loss: 0.7025 - accuracy: 0.8182 - val_loss: 0.9155 - val_accuracy: 0.7752 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "98/98 [==============================] - 31s 319ms/step - loss: 0.7126 - accuracy: 0.8131 - val_loss: 0.8550 - val_accuracy: 0.7789 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.6688 - accuracy: 0.8291 - val_loss: 0.8791 - val_accuracy: 0.7831 - lr: 5.0000e-04\n",
            "Epoch 83/500\n",
            "98/98 [==============================] - 32s 323ms/step - loss: 0.6524 - accuracy: 0.8329 - val_loss: 0.8668 - val_accuracy: 0.7853 - lr: 5.0000e-04\n",
            "Epoch 84/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.6478 - accuracy: 0.8344 - val_loss: 0.8696 - val_accuracy: 0.7777 - lr: 5.0000e-04\n",
            "Epoch 85/500\n",
            "98/98 [==============================] - 34s 343ms/step - loss: 0.6557 - accuracy: 0.8319 - val_loss: 0.9039 - val_accuracy: 0.7742 - lr: 5.0000e-04\n",
            "Epoch 86/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.6502 - accuracy: 0.8328 - val_loss: 0.9176 - val_accuracy: 0.7828 - lr: 5.0000e-04\n",
            "Epoch 87/500\n",
            "98/98 [==============================] - 31s 318ms/step - loss: 0.6455 - accuracy: 0.8349 - val_loss: 0.9022 - val_accuracy: 0.7747 - lr: 5.0000e-04\n",
            "Epoch 88/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.6546 - accuracy: 0.8311 - val_loss: 0.9045 - val_accuracy: 0.7809 - lr: 5.0000e-04\n",
            "Epoch 89/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.6503 - accuracy: 0.8323 - val_loss: 0.8430 - val_accuracy: 0.7869 - lr: 5.0000e-04\n",
            "Epoch 90/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.6464 - accuracy: 0.8330 - val_loss: 0.8884 - val_accuracy: 0.7858 - lr: 5.0000e-04\n",
            "Epoch 91/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.6415 - accuracy: 0.8356 - val_loss: 0.9865 - val_accuracy: 0.7686 - lr: 5.0000e-04\n",
            "Epoch 92/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.6444 - accuracy: 0.8332 - val_loss: 0.8737 - val_accuracy: 0.7838 - lr: 5.0000e-04\n",
            "Epoch 93/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.6422 - accuracy: 0.8340 - val_loss: 0.8413 - val_accuracy: 0.7883 - lr: 5.0000e-04\n",
            "Epoch 94/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.6408 - accuracy: 0.8357 - val_loss: 0.8493 - val_accuracy: 0.7879 - lr: 5.0000e-04\n",
            "Epoch 95/500\n",
            "98/98 [==============================] - 30s 311ms/step - loss: 0.6285 - accuracy: 0.8393 - val_loss: 0.9106 - val_accuracy: 0.7860 - lr: 5.0000e-04\n",
            "Epoch 96/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.6384 - accuracy: 0.8357 - val_loss: 0.9017 - val_accuracy: 0.7805 - lr: 5.0000e-04\n",
            "Epoch 97/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.6346 - accuracy: 0.8371 - val_loss: 0.9082 - val_accuracy: 0.7690 - lr: 5.0000e-04\n",
            "Epoch 98/500\n",
            "98/98 [==============================] - 32s 320ms/step - loss: 0.6435 - accuracy: 0.8346 - val_loss: 0.8912 - val_accuracy: 0.7829 - lr: 5.0000e-04\n",
            "Epoch 99/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.6308 - accuracy: 0.8385 - val_loss: 0.9238 - val_accuracy: 0.7864 - lr: 5.0000e-04\n",
            "Epoch 100/500\n",
            "98/98 [==============================] - 31s 319ms/step - loss: 0.6326 - accuracy: 0.8375 - val_loss: 0.9077 - val_accuracy: 0.7743 - lr: 5.0000e-04\n",
            "Epoch 101/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.6300 - accuracy: 0.8378 - val_loss: 0.8609 - val_accuracy: 0.7852 - lr: 5.0000e-04\n",
            "Epoch 102/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.6264 - accuracy: 0.8381 - val_loss: 0.8857 - val_accuracy: 0.7815 - lr: 5.0000e-04\n",
            "Epoch 103/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.6297 - accuracy: 0.8382 - val_loss: 0.8826 - val_accuracy: 0.7797 - lr: 5.0000e-04\n",
            "Epoch 104/500\n",
            "98/98 [==============================] - 32s 326ms/step - loss: 0.6266 - accuracy: 0.8393 - val_loss: 0.8916 - val_accuracy: 0.7821 - lr: 5.0000e-04\n",
            "Epoch 105/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.6285 - accuracy: 0.8376 - val_loss: 0.8497 - val_accuracy: 0.7855 - lr: 5.0000e-04\n",
            "Epoch 106/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.6309 - accuracy: 0.8372 - val_loss: 0.8568 - val_accuracy: 0.7822 - lr: 5.0000e-04\n",
            "Epoch 107/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.6224 - accuracy: 0.8412 - val_loss: 0.8574 - val_accuracy: 0.7839 - lr: 5.0000e-04\n",
            "Epoch 108/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.6165 - accuracy: 0.8420 - val_loss: 0.9241 - val_accuracy: 0.7787 - lr: 5.0000e-04\n",
            "Epoch 109/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.6235 - accuracy: 0.8402 - val_loss: 0.8449 - val_accuracy: 0.7862 - lr: 5.0000e-04\n",
            "Epoch 110/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.6166 - accuracy: 0.8421 - val_loss: 0.9370 - val_accuracy: 0.7779 - lr: 5.0000e-04\n",
            "Epoch 111/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.6242 - accuracy: 0.8403 - val_loss: 0.8548 - val_accuracy: 0.7864 - lr: 5.0000e-04\n",
            "Epoch 112/500\n",
            "98/98 [==============================] - 31s 319ms/step - loss: 0.6198 - accuracy: 0.8405 - val_loss: 0.9383 - val_accuracy: 0.7793 - lr: 5.0000e-04\n",
            "Epoch 113/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.6212 - accuracy: 0.8404 - val_loss: 0.9316 - val_accuracy: 0.7804 - lr: 5.0000e-04\n",
            "Epoch 114/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.6190 - accuracy: 0.8411 - val_loss: 0.8486 - val_accuracy: 0.7895 - lr: 5.0000e-04\n",
            "Epoch 115/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.6159 - accuracy: 0.8419 - val_loss: 0.8662 - val_accuracy: 0.7902 - lr: 5.0000e-04\n",
            "Epoch 116/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.6115 - accuracy: 0.8430 - val_loss: 0.9331 - val_accuracy: 0.7862 - lr: 5.0000e-04\n",
            "Epoch 117/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.6084 - accuracy: 0.8444 - val_loss: 0.8975 - val_accuracy: 0.7826 - lr: 5.0000e-04\n",
            "Epoch 118/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.6168 - accuracy: 0.8419 - val_loss: 0.8600 - val_accuracy: 0.7870 - lr: 5.0000e-04\n",
            "Epoch 119/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.6088 - accuracy: 0.8446 - val_loss: 0.9046 - val_accuracy: 0.7870 - lr: 5.0000e-04\n",
            "Epoch 120/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.6103 - accuracy: 0.8452 - val_loss: 0.9160 - val_accuracy: 0.7818 - lr: 5.0000e-04\n",
            "Epoch 121/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.6075 - accuracy: 0.8452 - val_loss: 0.9007 - val_accuracy: 0.7917 - lr: 5.0000e-04\n",
            "Epoch 122/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.6058 - accuracy: 0.8459 - val_loss: 0.8604 - val_accuracy: 0.7886 - lr: 5.0000e-04\n",
            "Epoch 123/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.6090 - accuracy: 0.8447 - val_loss: 0.8882 - val_accuracy: 0.7853 - lr: 5.0000e-04\n",
            "Epoch 124/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.6102 - accuracy: 0.8440 - val_loss: 0.9217 - val_accuracy: 0.7868 - lr: 5.0000e-04\n",
            "Epoch 125/500\n",
            "98/98 [==============================] - 32s 325ms/step - loss: 0.6002 - accuracy: 0.8469 - val_loss: 0.8889 - val_accuracy: 0.7844 - lr: 5.0000e-04\n",
            "Epoch 126/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.6030 - accuracy: 0.8457 - val_loss: 0.8489 - val_accuracy: 0.7931 - lr: 5.0000e-04\n",
            "Epoch 127/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.6012 - accuracy: 0.8462 - val_loss: 0.8843 - val_accuracy: 0.7909 - lr: 5.0000e-04\n",
            "Epoch 128/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.5988 - accuracy: 0.8478 - val_loss: 0.9288 - val_accuracy: 0.7826 - lr: 5.0000e-04\n",
            "Epoch 129/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.6053 - accuracy: 0.8449 - val_loss: 0.9191 - val_accuracy: 0.7828 - lr: 5.0000e-04\n",
            "Epoch 130/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5999 - accuracy: 0.8471 - val_loss: 0.9500 - val_accuracy: 0.7829 - lr: 5.0000e-04\n",
            "Epoch 131/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 0.6012 - accuracy: 0.8466 - val_loss: 0.9061 - val_accuracy: 0.7869 - lr: 5.0000e-04\n",
            "Epoch 132/500\n",
            "98/98 [==============================] - 33s 332ms/step - loss: 0.6013 - accuracy: 0.8462 - val_loss: 0.9412 - val_accuracy: 0.7825 - lr: 5.0000e-04\n",
            "Epoch 133/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.6006 - accuracy: 0.8472 - val_loss: 0.9075 - val_accuracy: 0.7868 - lr: 5.0000e-04\n",
            "Epoch 134/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5934 - accuracy: 0.8505 - val_loss: 0.8497 - val_accuracy: 0.7841 - lr: 5.0000e-04\n",
            "Epoch 135/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.5998 - accuracy: 0.8479 - val_loss: 0.9172 - val_accuracy: 0.7909 - lr: 5.0000e-04\n",
            "Epoch 136/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.5937 - accuracy: 0.8488 - val_loss: 0.8562 - val_accuracy: 0.7914 - lr: 5.0000e-04\n",
            "Epoch 137/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.5951 - accuracy: 0.8478 - val_loss: 0.8558 - val_accuracy: 0.7829 - lr: 5.0000e-04\n",
            "Epoch 138/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5965 - accuracy: 0.8492 - val_loss: 0.8537 - val_accuracy: 0.7923 - lr: 5.0000e-04\n",
            "Epoch 139/500\n",
            "98/98 [==============================] - 32s 326ms/step - loss: 0.5961 - accuracy: 0.8482 - val_loss: 0.9762 - val_accuracy: 0.7722 - lr: 5.0000e-04\n",
            "Epoch 140/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.5923 - accuracy: 0.8498 - val_loss: 0.9408 - val_accuracy: 0.7883 - lr: 5.0000e-04\n",
            "Epoch 141/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5960 - accuracy: 0.8481 - val_loss: 0.8991 - val_accuracy: 0.7763 - lr: 5.0000e-04\n",
            "Epoch 142/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5611 - accuracy: 0.8602 - val_loss: 0.8557 - val_accuracy: 0.7948 - lr: 1.0000e-04\n",
            "Epoch 143/500\n",
            "98/98 [==============================] - 31s 310ms/step - loss: 0.5487 - accuracy: 0.8635 - val_loss: 0.8677 - val_accuracy: 0.7928 - lr: 1.0000e-04\n",
            "Epoch 144/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5456 - accuracy: 0.8640 - val_loss: 0.8644 - val_accuracy: 0.7960 - lr: 1.0000e-04\n",
            "Epoch 145/500\n",
            "98/98 [==============================] - 31s 319ms/step - loss: 0.5464 - accuracy: 0.8637 - val_loss: 0.8645 - val_accuracy: 0.7980 - lr: 1.0000e-04\n",
            "Epoch 146/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5455 - accuracy: 0.8633 - val_loss: 0.8661 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 147/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5452 - accuracy: 0.8640 - val_loss: 0.8909 - val_accuracy: 0.7936 - lr: 1.0000e-04\n",
            "Epoch 148/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.5411 - accuracy: 0.8652 - val_loss: 0.8908 - val_accuracy: 0.7945 - lr: 1.0000e-04\n",
            "Epoch 149/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.5426 - accuracy: 0.8641 - val_loss: 0.8700 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 150/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5399 - accuracy: 0.8659 - val_loss: 0.8635 - val_accuracy: 0.7979 - lr: 1.0000e-04\n",
            "Epoch 151/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5423 - accuracy: 0.8640 - val_loss: 0.8740 - val_accuracy: 0.7968 - lr: 1.0000e-04\n",
            "Epoch 152/500\n",
            "98/98 [==============================] - 32s 328ms/step - loss: 0.5383 - accuracy: 0.8655 - val_loss: 0.8826 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
            "Epoch 153/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5412 - accuracy: 0.8640 - val_loss: 0.8644 - val_accuracy: 0.7952 - lr: 1.0000e-04\n",
            "Epoch 154/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.5415 - accuracy: 0.8643 - val_loss: 0.8549 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
            "Epoch 155/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.5365 - accuracy: 0.8662 - val_loss: 0.8741 - val_accuracy: 0.7965 - lr: 1.0000e-04\n",
            "Epoch 156/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5394 - accuracy: 0.8653 - val_loss: 0.8750 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 157/500\n",
            "98/98 [==============================] - 31s 318ms/step - loss: 0.5373 - accuracy: 0.8655 - val_loss: 0.8695 - val_accuracy: 0.7991 - lr: 1.0000e-04\n",
            "Epoch 158/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.5377 - accuracy: 0.8647 - val_loss: 0.8860 - val_accuracy: 0.7891 - lr: 1.0000e-04\n",
            "Epoch 159/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.5372 - accuracy: 0.8655 - val_loss: 0.8913 - val_accuracy: 0.7940 - lr: 1.0000e-04\n",
            "Epoch 160/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.5399 - accuracy: 0.8643 - val_loss: 0.8752 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 161/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 0.5358 - accuracy: 0.8658 - val_loss: 0.8751 - val_accuracy: 0.7951 - lr: 1.0000e-04\n",
            "Epoch 162/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 0.5389 - accuracy: 0.8644 - val_loss: 0.8813 - val_accuracy: 0.7925 - lr: 1.0000e-04\n",
            "Epoch 163/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5355 - accuracy: 0.8660 - val_loss: 0.8982 - val_accuracy: 0.7972 - lr: 1.0000e-04\n",
            "Epoch 164/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5337 - accuracy: 0.8664 - val_loss: 0.8541 - val_accuracy: 0.7930 - lr: 1.0000e-04\n",
            "Epoch 165/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5367 - accuracy: 0.8648 - val_loss: 0.8805 - val_accuracy: 0.7918 - lr: 1.0000e-04\n",
            "Epoch 166/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5329 - accuracy: 0.8661 - val_loss: 0.8761 - val_accuracy: 0.7992 - lr: 1.0000e-04\n",
            "Epoch 167/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.5388 - accuracy: 0.8646 - val_loss: 0.8731 - val_accuracy: 0.7969 - lr: 1.0000e-04\n",
            "Epoch 168/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.5313 - accuracy: 0.8669 - val_loss: 0.8680 - val_accuracy: 0.7993 - lr: 1.0000e-04\n",
            "Epoch 169/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 0.5324 - accuracy: 0.8665 - val_loss: 0.8760 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 170/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.5309 - accuracy: 0.8670 - val_loss: 0.8876 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 171/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 0.5339 - accuracy: 0.8650 - val_loss: 0.8693 - val_accuracy: 0.7978 - lr: 1.0000e-04\n",
            "Epoch 172/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 0.5306 - accuracy: 0.8661 - val_loss: 0.8534 - val_accuracy: 0.7989 - lr: 1.0000e-04\n",
            "Epoch 173/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 0.5307 - accuracy: 0.8669 - val_loss: 0.8785 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 174/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.5329 - accuracy: 0.8653 - val_loss: 0.8524 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
            "Epoch 175/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.5285 - accuracy: 0.8672 - val_loss: 0.8705 - val_accuracy: 0.7975 - lr: 1.0000e-04\n",
            "Epoch 176/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5281 - accuracy: 0.8666 - val_loss: 0.8840 - val_accuracy: 0.7970 - lr: 1.0000e-04\n",
            "Epoch 177/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.5319 - accuracy: 0.8658 - val_loss: 0.8799 - val_accuracy: 0.7989 - lr: 1.0000e-04\n",
            "Epoch 178/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5292 - accuracy: 0.8670 - val_loss: 0.9477 - val_accuracy: 0.7920 - lr: 1.0000e-04\n",
            "Epoch 179/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.5282 - accuracy: 0.8678 - val_loss: 0.8846 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 180/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.5297 - accuracy: 0.8672 - val_loss: 0.8974 - val_accuracy: 0.8003 - lr: 1.0000e-04\n",
            "Epoch 181/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.5272 - accuracy: 0.8681 - val_loss: 0.8375 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 182/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 0.5279 - accuracy: 0.8677 - val_loss: 0.9022 - val_accuracy: 0.7934 - lr: 1.0000e-04\n",
            "Epoch 183/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5280 - accuracy: 0.8676 - val_loss: 0.8685 - val_accuracy: 0.7968 - lr: 1.0000e-04\n",
            "Epoch 184/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5264 - accuracy: 0.8678 - val_loss: 0.8812 - val_accuracy: 0.7921 - lr: 1.0000e-04\n",
            "Epoch 185/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.5271 - accuracy: 0.8679 - val_loss: 0.8682 - val_accuracy: 0.7956 - lr: 1.0000e-04\n",
            "Epoch 186/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.5265 - accuracy: 0.8680 - val_loss: 0.8775 - val_accuracy: 0.7983 - lr: 1.0000e-04\n",
            "Epoch 187/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.5270 - accuracy: 0.8666 - val_loss: 0.8648 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 188/500\n",
            "98/98 [==============================] - 32s 329ms/step - loss: 0.5244 - accuracy: 0.8681 - val_loss: 0.8614 - val_accuracy: 0.7990 - lr: 1.0000e-04\n",
            "Epoch 189/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.5273 - accuracy: 0.8681 - val_loss: 0.8665 - val_accuracy: 0.7978 - lr: 1.0000e-04\n",
            "Epoch 190/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5239 - accuracy: 0.8686 - val_loss: 0.8619 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
            "Epoch 191/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.5239 - accuracy: 0.8688 - val_loss: 0.8636 - val_accuracy: 0.7985 - lr: 1.0000e-04\n",
            "Epoch 192/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5217 - accuracy: 0.8686 - val_loss: 0.8605 - val_accuracy: 0.7972 - lr: 1.0000e-04\n",
            "Epoch 193/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.5219 - accuracy: 0.8687 - val_loss: 0.8660 - val_accuracy: 0.7992 - lr: 1.0000e-04\n",
            "Epoch 194/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5239 - accuracy: 0.8682 - val_loss: 0.8636 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 195/500\n",
            "98/98 [==============================] - 32s 321ms/step - loss: 0.5206 - accuracy: 0.8695 - val_loss: 0.8764 - val_accuracy: 0.7969 - lr: 1.0000e-04\n",
            "Epoch 196/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.5231 - accuracy: 0.8682 - val_loss: 0.8570 - val_accuracy: 0.7962 - lr: 1.0000e-04\n",
            "Epoch 197/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.5227 - accuracy: 0.8690 - val_loss: 0.8703 - val_accuracy: 0.7982 - lr: 1.0000e-04\n",
            "Epoch 198/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.5223 - accuracy: 0.8691 - val_loss: 0.8568 - val_accuracy: 0.7984 - lr: 1.0000e-04\n",
            "Epoch 199/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5206 - accuracy: 0.8686 - val_loss: 0.8543 - val_accuracy: 0.7996 - lr: 1.0000e-04\n",
            "Epoch 200/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5233 - accuracy: 0.8679 - val_loss: 0.8884 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 201/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.5212 - accuracy: 0.8690 - val_loss: 0.8887 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 202/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5209 - accuracy: 0.8689 - val_loss: 0.8891 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 203/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5218 - accuracy: 0.8682 - val_loss: 0.8965 - val_accuracy: 0.7937 - lr: 1.0000e-04\n",
            "Epoch 204/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.5202 - accuracy: 0.8682 - val_loss: 0.8945 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 205/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 0.5176 - accuracy: 0.8701 - val_loss: 0.8799 - val_accuracy: 0.7991 - lr: 1.0000e-04\n",
            "Epoch 206/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 0.5200 - accuracy: 0.8693 - val_loss: 0.8603 - val_accuracy: 0.7950 - lr: 1.0000e-04\n",
            "Epoch 207/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 0.5220 - accuracy: 0.8678 - val_loss: 0.8790 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 208/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.5188 - accuracy: 0.8691 - val_loss: 0.8889 - val_accuracy: 0.7941 - lr: 1.0000e-04\n",
            "Epoch 209/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5190 - accuracy: 0.8687 - val_loss: 0.8925 - val_accuracy: 0.7972 - lr: 1.0000e-04\n",
            "Epoch 210/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.5207 - accuracy: 0.8694 - val_loss: 0.9361 - val_accuracy: 0.7898 - lr: 1.0000e-04\n",
            "Epoch 211/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.5223 - accuracy: 0.8682 - val_loss: 0.9085 - val_accuracy: 0.7907 - lr: 1.0000e-04\n",
            "Epoch 212/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5175 - accuracy: 0.8696 - val_loss: 0.8816 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 213/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 0.5178 - accuracy: 0.8693 - val_loss: 0.9050 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 214/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.5170 - accuracy: 0.8702 - val_loss: 0.8543 - val_accuracy: 0.8004 - lr: 1.0000e-04\n",
            "Epoch 215/500\n",
            "98/98 [==============================] - 31s 310ms/step - loss: 0.5181 - accuracy: 0.8688 - val_loss: 0.8950 - val_accuracy: 0.7957 - lr: 1.0000e-04\n",
            "Epoch 216/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5181 - accuracy: 0.8687 - val_loss: 0.8852 - val_accuracy: 0.7944 - lr: 1.0000e-04\n",
            "Epoch 217/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.5184 - accuracy: 0.8687 - val_loss: 0.8629 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
            "Epoch 218/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.5165 - accuracy: 0.8701 - val_loss: 0.8670 - val_accuracy: 0.7972 - lr: 1.0000e-04\n",
            "Epoch 219/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.5156 - accuracy: 0.8696 - val_loss: 0.8919 - val_accuracy: 0.7984 - lr: 1.0000e-04\n",
            "Epoch 220/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5146 - accuracy: 0.8706 - val_loss: 0.8790 - val_accuracy: 0.7969 - lr: 1.0000e-04\n",
            "Epoch 221/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 0.5166 - accuracy: 0.8698 - val_loss: 0.8958 - val_accuracy: 0.7975 - lr: 1.0000e-04\n",
            "Epoch 222/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.5185 - accuracy: 0.8689 - val_loss: 0.8887 - val_accuracy: 0.7931 - lr: 1.0000e-04\n",
            "Epoch 223/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5160 - accuracy: 0.8699 - val_loss: 0.8830 - val_accuracy: 0.7984 - lr: 1.0000e-04\n",
            "Epoch 224/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5152 - accuracy: 0.8690 - val_loss: 0.8610 - val_accuracy: 0.8001 - lr: 1.0000e-04\n",
            "Epoch 225/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.5150 - accuracy: 0.8693 - val_loss: 0.8815 - val_accuracy: 0.7951 - lr: 1.0000e-04\n",
            "Epoch 226/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5140 - accuracy: 0.8715 - val_loss: 0.8977 - val_accuracy: 0.7914 - lr: 1.0000e-04\n",
            "Epoch 227/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.5126 - accuracy: 0.8709 - val_loss: 0.8814 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 228/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.5142 - accuracy: 0.8702 - val_loss: 0.8721 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 229/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.5166 - accuracy: 0.8689 - val_loss: 0.8797 - val_accuracy: 0.7950 - lr: 1.0000e-04\n",
            "Epoch 230/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 0.5147 - accuracy: 0.8693 - val_loss: 0.8855 - val_accuracy: 0.7998 - lr: 1.0000e-04\n",
            "Epoch 231/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5158 - accuracy: 0.8699 - val_loss: 0.8564 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
            "Epoch 232/500\n",
            "98/98 [==============================] - 32s 328ms/step - loss: 0.5134 - accuracy: 0.8701 - val_loss: 0.9084 - val_accuracy: 0.7948 - lr: 1.0000e-04\n",
            "Epoch 233/500\n",
            "98/98 [==============================] - 32s 326ms/step - loss: 0.5141 - accuracy: 0.8708 - val_loss: 0.8745 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 234/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.5144 - accuracy: 0.8706 - val_loss: 0.8522 - val_accuracy: 0.7993 - lr: 1.0000e-04\n",
            "Epoch 235/500\n",
            "98/98 [==============================] - 32s 324ms/step - loss: 0.5151 - accuracy: 0.8696 - val_loss: 0.8727 - val_accuracy: 0.8001 - lr: 1.0000e-04\n",
            "Epoch 236/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.5136 - accuracy: 0.8701 - val_loss: 0.8773 - val_accuracy: 0.7979 - lr: 1.0000e-04\n",
            "Epoch 237/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.5123 - accuracy: 0.8701 - val_loss: 0.9266 - val_accuracy: 0.7909 - lr: 1.0000e-04\n",
            "Epoch 238/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.5129 - accuracy: 0.8706 - val_loss: 0.8745 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 239/500\n",
            "98/98 [==============================] - 33s 336ms/step - loss: 0.5119 - accuracy: 0.8704 - val_loss: 0.9055 - val_accuracy: 0.7943 - lr: 1.0000e-04\n",
            "Epoch 240/500\n",
            "98/98 [==============================] - 32s 321ms/step - loss: 0.5118 - accuracy: 0.8703 - val_loss: 0.8841 - val_accuracy: 0.7943 - lr: 1.0000e-04\n",
            "Epoch 241/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.5082 - accuracy: 0.8719 - val_loss: 0.8637 - val_accuracy: 0.8005 - lr: 1.0000e-04\n",
            "Epoch 242/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.5087 - accuracy: 0.8715 - val_loss: 0.8784 - val_accuracy: 0.7938 - lr: 1.0000e-04\n",
            "Epoch 243/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.5099 - accuracy: 0.8708 - val_loss: 0.8876 - val_accuracy: 0.8001 - lr: 1.0000e-04\n",
            "Epoch 244/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5119 - accuracy: 0.8704 - val_loss: 0.8889 - val_accuracy: 0.7991 - lr: 1.0000e-04\n",
            "Epoch 245/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5133 - accuracy: 0.8699 - val_loss: 0.8835 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 246/500\n",
            "98/98 [==============================] - 31s 319ms/step - loss: 0.5067 - accuracy: 0.8720 - val_loss: 0.8552 - val_accuracy: 0.7998 - lr: 1.0000e-04\n",
            "Epoch 247/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5126 - accuracy: 0.8703 - val_loss: 0.9017 - val_accuracy: 0.8007 - lr: 1.0000e-04\n",
            "Epoch 248/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.5101 - accuracy: 0.8710 - val_loss: 0.8743 - val_accuracy: 0.7966 - lr: 1.0000e-04\n",
            "Epoch 249/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5098 - accuracy: 0.8710 - val_loss: 0.8773 - val_accuracy: 0.7985 - lr: 1.0000e-04\n",
            "Epoch 250/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 0.5099 - accuracy: 0.8710 - val_loss: 0.9040 - val_accuracy: 0.7947 - lr: 1.0000e-04\n",
            "Epoch 251/500\n",
            "98/98 [==============================] - 30s 300ms/step - loss: 0.5084 - accuracy: 0.8715 - val_loss: 0.9208 - val_accuracy: 0.7958 - lr: 1.0000e-04\n",
            "Epoch 252/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5105 - accuracy: 0.8704 - val_loss: 0.8887 - val_accuracy: 0.7917 - lr: 1.0000e-04\n",
            "Epoch 253/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5049 - accuracy: 0.8722 - val_loss: 0.8854 - val_accuracy: 0.7948 - lr: 1.0000e-04\n",
            "Epoch 254/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5082 - accuracy: 0.8720 - val_loss: 0.8890 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 255/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5106 - accuracy: 0.8703 - val_loss: 0.8561 - val_accuracy: 0.7975 - lr: 1.0000e-04\n",
            "Epoch 256/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.5085 - accuracy: 0.8714 - val_loss: 0.8770 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
            "Epoch 257/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.5067 - accuracy: 0.8717 - val_loss: 0.9121 - val_accuracy: 0.7950 - lr: 1.0000e-04\n",
            "Epoch 258/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.5078 - accuracy: 0.8717 - val_loss: 0.9166 - val_accuracy: 0.7934 - lr: 1.0000e-04\n",
            "Epoch 259/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5082 - accuracy: 0.8708 - val_loss: 0.8856 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
            "Epoch 260/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.5066 - accuracy: 0.8717 - val_loss: 0.8906 - val_accuracy: 0.7930 - lr: 1.0000e-04\n",
            "Epoch 261/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.5100 - accuracy: 0.8706 - val_loss: 0.8729 - val_accuracy: 0.8009 - lr: 1.0000e-04\n",
            "Epoch 262/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5082 - accuracy: 0.8712 - val_loss: 0.8793 - val_accuracy: 0.7999 - lr: 1.0000e-04\n",
            "Epoch 263/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5037 - accuracy: 0.8732 - val_loss: 0.9083 - val_accuracy: 0.7944 - lr: 1.0000e-04\n",
            "Epoch 264/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.5061 - accuracy: 0.8712 - val_loss: 0.9052 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 265/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.5088 - accuracy: 0.8711 - val_loss: 0.8923 - val_accuracy: 0.7956 - lr: 1.0000e-04\n",
            "Epoch 266/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.5077 - accuracy: 0.8712 - val_loss: 0.9025 - val_accuracy: 0.7925 - lr: 1.0000e-04\n",
            "Epoch 267/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.5060 - accuracy: 0.8716 - val_loss: 0.8976 - val_accuracy: 0.7921 - lr: 1.0000e-04\n",
            "Epoch 268/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 0.5083 - accuracy: 0.8706 - val_loss: 0.8718 - val_accuracy: 0.7984 - lr: 1.0000e-04\n",
            "Epoch 269/500\n",
            "98/98 [==============================] - 30s 300ms/step - loss: 0.5029 - accuracy: 0.8727 - val_loss: 0.9024 - val_accuracy: 0.7959 - lr: 1.0000e-04\n",
            "Epoch 270/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.5081 - accuracy: 0.8704 - val_loss: 0.8853 - val_accuracy: 0.7946 - lr: 1.0000e-04\n",
            "Epoch 271/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.5023 - accuracy: 0.8722 - val_loss: 0.8935 - val_accuracy: 0.8009 - lr: 1.0000e-04\n",
            "Epoch 272/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5063 - accuracy: 0.8719 - val_loss: 0.8950 - val_accuracy: 0.7923 - lr: 1.0000e-04\n",
            "Epoch 273/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5043 - accuracy: 0.8723 - val_loss: 0.8624 - val_accuracy: 0.7993 - lr: 1.0000e-04\n",
            "Epoch 274/500\n",
            "98/98 [==============================] - 32s 323ms/step - loss: 0.5056 - accuracy: 0.8709 - val_loss: 0.8608 - val_accuracy: 0.8003 - lr: 1.0000e-04\n",
            "Epoch 275/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.5031 - accuracy: 0.8727 - val_loss: 0.9081 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 276/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.5070 - accuracy: 0.8704 - val_loss: 0.8944 - val_accuracy: 0.7956 - lr: 1.0000e-04\n",
            "Epoch 277/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.5056 - accuracy: 0.8719 - val_loss: 0.8840 - val_accuracy: 0.7979 - lr: 1.0000e-04\n",
            "Epoch 278/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5049 - accuracy: 0.8718 - val_loss: 0.8865 - val_accuracy: 0.8004 - lr: 1.0000e-04\n",
            "Epoch 279/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.5031 - accuracy: 0.8725 - val_loss: 0.8627 - val_accuracy: 0.7984 - lr: 1.0000e-04\n",
            "Epoch 280/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.5015 - accuracy: 0.8739 - val_loss: 0.8951 - val_accuracy: 0.7968 - lr: 1.0000e-04\n",
            "Epoch 281/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.5038 - accuracy: 0.8716 - val_loss: 0.8861 - val_accuracy: 0.7923 - lr: 1.0000e-04\n",
            "Epoch 282/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.5030 - accuracy: 0.8729 - val_loss: 0.9324 - val_accuracy: 0.7943 - lr: 1.0000e-04\n",
            "Epoch 283/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5036 - accuracy: 0.8721 - val_loss: 0.8854 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 284/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5022 - accuracy: 0.8732 - val_loss: 0.8965 - val_accuracy: 0.7939 - lr: 1.0000e-04\n",
            "Epoch 285/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.5011 - accuracy: 0.8728 - val_loss: 0.8919 - val_accuracy: 0.7950 - lr: 1.0000e-04\n",
            "Epoch 286/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5046 - accuracy: 0.8726 - val_loss: 0.8836 - val_accuracy: 0.7987 - lr: 1.0000e-04\n",
            "Epoch 287/500\n",
            "98/98 [==============================] - 33s 335ms/step - loss: 0.4999 - accuracy: 0.8737 - val_loss: 0.9116 - val_accuracy: 0.7976 - lr: 1.0000e-04\n",
            "Epoch 288/500\n",
            "98/98 [==============================] - 31s 319ms/step - loss: 0.5016 - accuracy: 0.8728 - val_loss: 0.8770 - val_accuracy: 0.8039 - lr: 1.0000e-04\n",
            "Epoch 289/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.5018 - accuracy: 0.8742 - val_loss: 0.8783 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 290/500\n",
            "98/98 [==============================] - 31s 318ms/step - loss: 0.5005 - accuracy: 0.8731 - val_loss: 0.8899 - val_accuracy: 0.7957 - lr: 1.0000e-04\n",
            "Epoch 291/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.5012 - accuracy: 0.8740 - val_loss: 0.9075 - val_accuracy: 0.7960 - lr: 1.0000e-04\n",
            "Epoch 292/500\n",
            "98/98 [==============================] - 32s 325ms/step - loss: 0.5014 - accuracy: 0.8721 - val_loss: 0.9349 - val_accuracy: 0.7958 - lr: 1.0000e-04\n",
            "Epoch 293/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.5014 - accuracy: 0.8725 - val_loss: 0.8971 - val_accuracy: 0.7924 - lr: 1.0000e-04\n",
            "Epoch 294/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5001 - accuracy: 0.8727 - val_loss: 0.8679 - val_accuracy: 0.8007 - lr: 1.0000e-04\n",
            "Epoch 295/500\n",
            "98/98 [==============================] - 32s 323ms/step - loss: 0.4998 - accuracy: 0.8735 - val_loss: 0.8859 - val_accuracy: 0.8004 - lr: 1.0000e-04\n",
            "Epoch 296/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 0.5029 - accuracy: 0.8719 - val_loss: 0.8795 - val_accuracy: 0.7982 - lr: 1.0000e-04\n",
            "Epoch 297/500\n",
            "98/98 [==============================] - 32s 324ms/step - loss: 0.5007 - accuracy: 0.8726 - val_loss: 0.8997 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
            "Epoch 298/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.4993 - accuracy: 0.8732 - val_loss: 0.8835 - val_accuracy: 0.7986 - lr: 1.0000e-04\n",
            "Epoch 299/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.5004 - accuracy: 0.8729 - val_loss: 0.8710 - val_accuracy: 0.8004 - lr: 1.0000e-04\n",
            "Epoch 300/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.4993 - accuracy: 0.8740 - val_loss: 0.8980 - val_accuracy: 0.7965 - lr: 1.0000e-04\n",
            "Epoch 301/500\n",
            "98/98 [==============================] - 32s 319ms/step - loss: 0.4990 - accuracy: 0.8739 - val_loss: 0.9329 - val_accuracy: 0.7931 - lr: 1.0000e-04\n",
            "Epoch 302/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.5019 - accuracy: 0.8721 - val_loss: 0.8795 - val_accuracy: 0.7970 - lr: 1.0000e-04\n",
            "Epoch 303/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.5007 - accuracy: 0.8728 - val_loss: 0.8830 - val_accuracy: 0.7960 - lr: 1.0000e-04\n",
            "Epoch 304/500\n",
            "98/98 [==============================] - 31s 319ms/step - loss: 0.4999 - accuracy: 0.8740 - val_loss: 0.8796 - val_accuracy: 0.7975 - lr: 1.0000e-04\n",
            "Epoch 305/500\n",
            "98/98 [==============================] - 31s 318ms/step - loss: 0.5015 - accuracy: 0.8721 - val_loss: 0.8747 - val_accuracy: 0.7983 - lr: 1.0000e-04\n",
            "Epoch 306/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4988 - accuracy: 0.8735 - val_loss: 0.8905 - val_accuracy: 0.7958 - lr: 1.0000e-04\n",
            "Epoch 307/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4991 - accuracy: 0.8743 - val_loss: 0.8937 - val_accuracy: 0.7990 - lr: 1.0000e-04\n",
            "Epoch 308/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4996 - accuracy: 0.8736 - val_loss: 0.9324 - val_accuracy: 0.7967 - lr: 1.0000e-04\n",
            "Epoch 309/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4972 - accuracy: 0.8737 - val_loss: 0.8710 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 310/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.5011 - accuracy: 0.8733 - val_loss: 0.8807 - val_accuracy: 0.7950 - lr: 1.0000e-04\n",
            "Epoch 311/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4987 - accuracy: 0.8729 - val_loss: 0.8976 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 312/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.5007 - accuracy: 0.8725 - val_loss: 0.9393 - val_accuracy: 0.8005 - lr: 1.0000e-04\n",
            "Epoch 313/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4980 - accuracy: 0.8736 - val_loss: 0.9140 - val_accuracy: 0.7946 - lr: 1.0000e-04\n",
            "Epoch 314/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4971 - accuracy: 0.8739 - val_loss: 0.9512 - val_accuracy: 0.7941 - lr: 1.0000e-04\n",
            "Epoch 315/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4967 - accuracy: 0.8741 - val_loss: 0.8725 - val_accuracy: 0.7994 - lr: 1.0000e-04\n",
            "Epoch 316/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4949 - accuracy: 0.8741 - val_loss: 0.8580 - val_accuracy: 0.8020 - lr: 1.0000e-04\n",
            "Epoch 317/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.4960 - accuracy: 0.8738 - val_loss: 0.8855 - val_accuracy: 0.7968 - lr: 1.0000e-04\n",
            "Epoch 318/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.4980 - accuracy: 0.8735 - val_loss: 0.8909 - val_accuracy: 0.7915 - lr: 1.0000e-04\n",
            "Epoch 319/500\n",
            "98/98 [==============================] - 31s 309ms/step - loss: 0.4973 - accuracy: 0.8743 - val_loss: 0.8848 - val_accuracy: 0.7965 - lr: 1.0000e-04\n",
            "Epoch 320/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4981 - accuracy: 0.8726 - val_loss: 0.8702 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 321/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 0.4964 - accuracy: 0.8734 - val_loss: 0.9113 - val_accuracy: 0.7947 - lr: 1.0000e-04\n",
            "Epoch 322/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4963 - accuracy: 0.8741 - val_loss: 0.8674 - val_accuracy: 0.7988 - lr: 1.0000e-04\n",
            "Epoch 323/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4986 - accuracy: 0.8722 - val_loss: 0.8722 - val_accuracy: 0.7989 - lr: 1.0000e-04\n",
            "Epoch 324/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 0.5004 - accuracy: 0.8719 - val_loss: 0.8627 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 325/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4945 - accuracy: 0.8745 - val_loss: 0.8934 - val_accuracy: 0.7970 - lr: 1.0000e-04\n",
            "Epoch 326/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.4974 - accuracy: 0.8737 - val_loss: 0.9022 - val_accuracy: 0.7987 - lr: 1.0000e-04\n",
            "Epoch 327/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4952 - accuracy: 0.8741 - val_loss: 0.8790 - val_accuracy: 0.7985 - lr: 1.0000e-04\n",
            "Epoch 328/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4948 - accuracy: 0.8748 - val_loss: 0.8914 - val_accuracy: 0.7948 - lr: 1.0000e-04\n",
            "Epoch 329/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.4934 - accuracy: 0.8740 - val_loss: 0.8975 - val_accuracy: 0.7957 - lr: 1.0000e-04\n",
            "Epoch 330/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4966 - accuracy: 0.8730 - val_loss: 0.8939 - val_accuracy: 0.7957 - lr: 1.0000e-04\n",
            "Epoch 331/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4939 - accuracy: 0.8740 - val_loss: 0.9138 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 332/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.4960 - accuracy: 0.8731 - val_loss: 0.8704 - val_accuracy: 0.7961 - lr: 1.0000e-04\n",
            "Epoch 333/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4931 - accuracy: 0.8752 - val_loss: 0.8810 - val_accuracy: 0.8017 - lr: 1.0000e-04\n",
            "Epoch 334/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4904 - accuracy: 0.8753 - val_loss: 0.9622 - val_accuracy: 0.7927 - lr: 1.0000e-04\n",
            "Epoch 335/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.4947 - accuracy: 0.8746 - val_loss: 0.8829 - val_accuracy: 0.7929 - lr: 1.0000e-04\n",
            "Epoch 336/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4931 - accuracy: 0.8752 - val_loss: 0.8872 - val_accuracy: 0.7986 - lr: 1.0000e-04\n",
            "Epoch 337/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4935 - accuracy: 0.8748 - val_loss: 0.8785 - val_accuracy: 0.7952 - lr: 1.0000e-04\n",
            "Epoch 338/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.4941 - accuracy: 0.8739 - val_loss: 0.9151 - val_accuracy: 0.7982 - lr: 1.0000e-04\n",
            "Epoch 339/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4928 - accuracy: 0.8744 - val_loss: 0.8783 - val_accuracy: 0.7983 - lr: 1.0000e-04\n",
            "Epoch 340/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4896 - accuracy: 0.8757 - val_loss: 0.8747 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 341/500\n",
            "98/98 [==============================] - 32s 328ms/step - loss: 0.4938 - accuracy: 0.8747 - val_loss: 0.8903 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 342/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4925 - accuracy: 0.8750 - val_loss: 0.8870 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 343/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4959 - accuracy: 0.8725 - val_loss: 0.8925 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
            "Epoch 344/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.4914 - accuracy: 0.8749 - val_loss: 0.9043 - val_accuracy: 0.7965 - lr: 1.0000e-04\n",
            "Epoch 345/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4945 - accuracy: 0.8741 - val_loss: 0.8487 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
            "Epoch 346/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4929 - accuracy: 0.8747 - val_loss: 0.8906 - val_accuracy: 0.7978 - lr: 1.0000e-04\n",
            "Epoch 347/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.4926 - accuracy: 0.8745 - val_loss: 0.8952 - val_accuracy: 0.7992 - lr: 1.0000e-04\n",
            "Epoch 348/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.4947 - accuracy: 0.8734 - val_loss: 0.8565 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 349/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4905 - accuracy: 0.8752 - val_loss: 0.8801 - val_accuracy: 0.8006 - lr: 1.0000e-04\n",
            "Epoch 350/500\n",
            "98/98 [==============================] - 31s 310ms/step - loss: 0.4970 - accuracy: 0.8737 - val_loss: 0.8458 - val_accuracy: 0.7985 - lr: 1.0000e-04\n",
            "Epoch 351/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.4889 - accuracy: 0.8754 - val_loss: 0.9170 - val_accuracy: 0.7968 - lr: 1.0000e-04\n",
            "Epoch 352/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4895 - accuracy: 0.8755 - val_loss: 0.8987 - val_accuracy: 0.7942 - lr: 1.0000e-04\n",
            "Epoch 353/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4921 - accuracy: 0.8753 - val_loss: 0.9314 - val_accuracy: 0.7958 - lr: 1.0000e-04\n",
            "Epoch 354/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4898 - accuracy: 0.8754 - val_loss: 0.9070 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 355/500\n",
            "98/98 [==============================] - 32s 326ms/step - loss: 0.4898 - accuracy: 0.8753 - val_loss: 0.9001 - val_accuracy: 0.7934 - lr: 1.0000e-04\n",
            "Epoch 356/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4916 - accuracy: 0.8748 - val_loss: 0.8923 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 357/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.4927 - accuracy: 0.8747 - val_loss: 0.9495 - val_accuracy: 0.7928 - lr: 1.0000e-04\n",
            "Epoch 358/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.4889 - accuracy: 0.8760 - val_loss: 0.8723 - val_accuracy: 0.8002 - lr: 1.0000e-04\n",
            "Epoch 359/500\n",
            "98/98 [==============================] - 30s 311ms/step - loss: 0.4893 - accuracy: 0.8759 - val_loss: 0.8944 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 360/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.4909 - accuracy: 0.8746 - val_loss: 0.8914 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 361/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.4899 - accuracy: 0.8752 - val_loss: 0.9045 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 362/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4885 - accuracy: 0.8762 - val_loss: 0.9086 - val_accuracy: 0.7966 - lr: 1.0000e-04\n",
            "Epoch 363/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4898 - accuracy: 0.8761 - val_loss: 0.8711 - val_accuracy: 0.7979 - lr: 1.0000e-04\n",
            "Epoch 364/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.4925 - accuracy: 0.8743 - val_loss: 0.8875 - val_accuracy: 0.7922 - lr: 1.0000e-04\n",
            "Epoch 365/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4936 - accuracy: 0.8744 - val_loss: 0.8814 - val_accuracy: 0.7970 - lr: 1.0000e-04\n",
            "Epoch 366/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4900 - accuracy: 0.8753 - val_loss: 0.8675 - val_accuracy: 0.7975 - lr: 1.0000e-04\n",
            "Epoch 367/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.4915 - accuracy: 0.8758 - val_loss: 0.9113 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 368/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4903 - accuracy: 0.8748 - val_loss: 0.8734 - val_accuracy: 0.7980 - lr: 1.0000e-04\n",
            "Epoch 369/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.4906 - accuracy: 0.8753 - val_loss: 0.9359 - val_accuracy: 0.7941 - lr: 1.0000e-04\n",
            "Epoch 370/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4900 - accuracy: 0.8760 - val_loss: 0.8941 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 371/500\n",
            "98/98 [==============================] - 31s 314ms/step - loss: 0.4874 - accuracy: 0.8763 - val_loss: 0.9243 - val_accuracy: 0.7940 - lr: 1.0000e-04\n",
            "Epoch 372/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.4874 - accuracy: 0.8757 - val_loss: 0.9056 - val_accuracy: 0.7938 - lr: 1.0000e-04\n",
            "Epoch 373/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4884 - accuracy: 0.8752 - val_loss: 0.8891 - val_accuracy: 0.7983 - lr: 1.0000e-04\n",
            "Epoch 374/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4887 - accuracy: 0.8755 - val_loss: 0.8738 - val_accuracy: 0.7980 - lr: 1.0000e-04\n",
            "Epoch 375/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4924 - accuracy: 0.8742 - val_loss: 0.8851 - val_accuracy: 0.7997 - lr: 1.0000e-04\n",
            "Epoch 376/500\n",
            "98/98 [==============================] - 32s 325ms/step - loss: 0.4908 - accuracy: 0.8743 - val_loss: 0.8836 - val_accuracy: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 377/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4912 - accuracy: 0.8751 - val_loss: 0.8769 - val_accuracy: 0.7960 - lr: 1.0000e-04\n",
            "Epoch 378/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4886 - accuracy: 0.8762 - val_loss: 0.9054 - val_accuracy: 0.7941 - lr: 1.0000e-04\n",
            "Epoch 379/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4877 - accuracy: 0.8756 - val_loss: 0.8980 - val_accuracy: 0.7947 - lr: 1.0000e-04\n",
            "Epoch 380/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.4883 - accuracy: 0.8757 - val_loss: 0.9387 - val_accuracy: 0.7926 - lr: 1.0000e-04\n",
            "Epoch 381/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.4867 - accuracy: 0.8759 - val_loss: 0.8865 - val_accuracy: 0.7962 - lr: 1.0000e-04\n",
            "Epoch 382/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4877 - accuracy: 0.8760 - val_loss: 0.9059 - val_accuracy: 0.7945 - lr: 1.0000e-04\n",
            "Epoch 383/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.4839 - accuracy: 0.8780 - val_loss: 0.9133 - val_accuracy: 0.7956 - lr: 1.0000e-04\n",
            "Epoch 384/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 0.4884 - accuracy: 0.8752 - val_loss: 0.8686 - val_accuracy: 0.7903 - lr: 1.0000e-04\n",
            "Epoch 385/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4893 - accuracy: 0.8751 - val_loss: 0.9274 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 386/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4871 - accuracy: 0.8760 - val_loss: 0.8975 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
            "Epoch 387/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4875 - accuracy: 0.8757 - val_loss: 0.8977 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 388/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.4892 - accuracy: 0.8757 - val_loss: 0.9009 - val_accuracy: 0.7960 - lr: 1.0000e-04\n",
            "Epoch 389/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 0.4852 - accuracy: 0.8766 - val_loss: 0.8997 - val_accuracy: 0.8003 - lr: 1.0000e-04\n",
            "Epoch 390/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4861 - accuracy: 0.8757 - val_loss: 0.9047 - val_accuracy: 0.7940 - lr: 1.0000e-04\n",
            "Epoch 391/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4854 - accuracy: 0.8760 - val_loss: 0.8887 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 392/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.4843 - accuracy: 0.8771 - val_loss: 0.8997 - val_accuracy: 0.7939 - lr: 1.0000e-04\n",
            "Epoch 393/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4864 - accuracy: 0.8757 - val_loss: 0.8801 - val_accuracy: 0.7993 - lr: 1.0000e-04\n",
            "Epoch 394/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4876 - accuracy: 0.8750 - val_loss: 0.9195 - val_accuracy: 0.7930 - lr: 1.0000e-04\n",
            "Epoch 395/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4871 - accuracy: 0.8768 - val_loss: 0.8819 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
            "Epoch 396/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4868 - accuracy: 0.8749 - val_loss: 0.9023 - val_accuracy: 0.8022 - lr: 1.0000e-04\n",
            "Epoch 397/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4898 - accuracy: 0.8749 - val_loss: 0.9105 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
            "Epoch 398/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.4847 - accuracy: 0.8758 - val_loss: 0.9048 - val_accuracy: 0.7937 - lr: 1.0000e-04\n",
            "Epoch 399/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4846 - accuracy: 0.8769 - val_loss: 0.8942 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
            "Epoch 400/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.4841 - accuracy: 0.8768 - val_loss: 0.9159 - val_accuracy: 0.7957 - lr: 1.0000e-04\n",
            "Epoch 401/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.4835 - accuracy: 0.8767 - val_loss: 0.9044 - val_accuracy: 0.7987 - lr: 1.0000e-04\n",
            "Epoch 402/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4826 - accuracy: 0.8767 - val_loss: 0.9184 - val_accuracy: 0.7963 - lr: 1.0000e-04\n",
            "Epoch 403/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 0.4864 - accuracy: 0.8753 - val_loss: 0.8901 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
            "Epoch 404/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4875 - accuracy: 0.8757 - val_loss: 0.9498 - val_accuracy: 0.7996 - lr: 1.0000e-04\n",
            "Epoch 405/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.4828 - accuracy: 0.8772 - val_loss: 0.9028 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 406/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4843 - accuracy: 0.8765 - val_loss: 0.8824 - val_accuracy: 0.7940 - lr: 1.0000e-04\n",
            "Epoch 407/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4832 - accuracy: 0.8767 - val_loss: 0.8833 - val_accuracy: 0.7967 - lr: 1.0000e-04\n",
            "Epoch 408/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4830 - accuracy: 0.8773 - val_loss: 0.9245 - val_accuracy: 0.7932 - lr: 1.0000e-04\n",
            "Epoch 409/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4838 - accuracy: 0.8769 - val_loss: 0.9160 - val_accuracy: 0.7981 - lr: 1.0000e-04\n",
            "Epoch 410/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4857 - accuracy: 0.8755 - val_loss: 0.9034 - val_accuracy: 0.7980 - lr: 1.0000e-04\n",
            "Epoch 411/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 0.4840 - accuracy: 0.8768 - val_loss: 0.9017 - val_accuracy: 0.7943 - lr: 1.0000e-04\n",
            "Epoch 412/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.4841 - accuracy: 0.8769 - val_loss: 0.9043 - val_accuracy: 0.7962 - lr: 1.0000e-04\n",
            "Epoch 413/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4859 - accuracy: 0.8761 - val_loss: 0.9144 - val_accuracy: 0.7933 - lr: 1.0000e-04\n",
            "Epoch 414/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.4845 - accuracy: 0.8761 - val_loss: 0.8875 - val_accuracy: 0.7962 - lr: 1.0000e-04\n",
            "Epoch 415/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4841 - accuracy: 0.8763 - val_loss: 0.8931 - val_accuracy: 0.7975 - lr: 1.0000e-04\n",
            "Epoch 416/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.4836 - accuracy: 0.8766 - val_loss: 0.9237 - val_accuracy: 0.7969 - lr: 1.0000e-04\n",
            "Epoch 417/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 0.4845 - accuracy: 0.8764 - val_loss: 0.8672 - val_accuracy: 0.8014 - lr: 1.0000e-04\n",
            "Epoch 418/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4860 - accuracy: 0.8758 - val_loss: 0.9373 - val_accuracy: 0.7921 - lr: 1.0000e-04\n",
            "Epoch 419/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.4833 - accuracy: 0.8772 - val_loss: 0.8993 - val_accuracy: 0.7970 - lr: 1.0000e-04\n",
            "Epoch 420/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4823 - accuracy: 0.8770 - val_loss: 0.8783 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
            "Epoch 421/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4825 - accuracy: 0.8770 - val_loss: 0.9244 - val_accuracy: 0.7941 - lr: 1.0000e-04\n",
            "Epoch 422/500\n",
            "98/98 [==============================] - 30s 300ms/step - loss: 0.4839 - accuracy: 0.8768 - val_loss: 0.8850 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 423/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.4821 - accuracy: 0.8765 - val_loss: 0.9389 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 424/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 0.4842 - accuracy: 0.8766 - val_loss: 0.8737 - val_accuracy: 0.8022 - lr: 1.0000e-04\n",
            "Epoch 425/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4826 - accuracy: 0.8774 - val_loss: 0.9107 - val_accuracy: 0.7943 - lr: 1.0000e-04\n",
            "Epoch 426/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4828 - accuracy: 0.8764 - val_loss: 0.8687 - val_accuracy: 0.7995 - lr: 1.0000e-04\n",
            "Epoch 427/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4837 - accuracy: 0.8762 - val_loss: 0.8960 - val_accuracy: 0.7983 - lr: 1.0000e-04\n",
            "Epoch 428/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.4839 - accuracy: 0.8763 - val_loss: 0.8874 - val_accuracy: 0.8016 - lr: 1.0000e-04\n",
            "Epoch 429/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4783 - accuracy: 0.8792 - val_loss: 0.8840 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 430/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4816 - accuracy: 0.8770 - val_loss: 0.9265 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
            "Epoch 431/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4815 - accuracy: 0.8774 - val_loss: 0.8989 - val_accuracy: 0.8016 - lr: 1.0000e-04\n",
            "Epoch 432/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4826 - accuracy: 0.8768 - val_loss: 0.9209 - val_accuracy: 0.7969 - lr: 1.0000e-04\n",
            "Epoch 433/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.4828 - accuracy: 0.8764 - val_loss: 0.9089 - val_accuracy: 0.7989 - lr: 1.0000e-04\n",
            "Epoch 434/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.4794 - accuracy: 0.8781 - val_loss: 0.8731 - val_accuracy: 0.7932 - lr: 1.0000e-04\n",
            "Epoch 435/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.4800 - accuracy: 0.8774 - val_loss: 0.8817 - val_accuracy: 0.7978 - lr: 1.0000e-04\n",
            "Epoch 436/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 0.4824 - accuracy: 0.8769 - val_loss: 0.9156 - val_accuracy: 0.7930 - lr: 1.0000e-04\n",
            "Epoch 437/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4821 - accuracy: 0.8771 - val_loss: 0.8981 - val_accuracy: 0.7966 - lr: 1.0000e-04\n",
            "Epoch 438/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4801 - accuracy: 0.8770 - val_loss: 0.9368 - val_accuracy: 0.7951 - lr: 1.0000e-04\n",
            "Epoch 439/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4810 - accuracy: 0.8760 - val_loss: 0.9040 - val_accuracy: 0.7992 - lr: 1.0000e-04\n",
            "Epoch 440/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.4823 - accuracy: 0.8769 - val_loss: 0.8949 - val_accuracy: 0.7944 - lr: 1.0000e-04\n",
            "Epoch 441/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4806 - accuracy: 0.8764 - val_loss: 0.9162 - val_accuracy: 0.7951 - lr: 1.0000e-04\n",
            "Epoch 442/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4825 - accuracy: 0.8771 - val_loss: 0.9140 - val_accuracy: 0.8005 - lr: 1.0000e-04\n",
            "Epoch 443/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.4806 - accuracy: 0.8771 - val_loss: 0.8764 - val_accuracy: 0.7967 - lr: 1.0000e-04\n",
            "Epoch 444/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4814 - accuracy: 0.8771 - val_loss: 0.8794 - val_accuracy: 0.7966 - lr: 1.0000e-04\n",
            "Epoch 445/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4803 - accuracy: 0.8778 - val_loss: 0.9326 - val_accuracy: 0.7902 - lr: 1.0000e-04\n",
            "Epoch 446/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4794 - accuracy: 0.8779 - val_loss: 0.9611 - val_accuracy: 0.7900 - lr: 1.0000e-04\n",
            "Epoch 447/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.4790 - accuracy: 0.8777 - val_loss: 0.9033 - val_accuracy: 0.7937 - lr: 1.0000e-04\n",
            "Epoch 448/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4799 - accuracy: 0.8777 - val_loss: 0.8683 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 449/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4788 - accuracy: 0.8777 - val_loss: 0.9302 - val_accuracy: 0.7934 - lr: 1.0000e-04\n",
            "Epoch 450/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4799 - accuracy: 0.8774 - val_loss: 0.9425 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 451/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4787 - accuracy: 0.8773 - val_loss: 0.9150 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 452/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 0.4798 - accuracy: 0.8774 - val_loss: 0.8814 - val_accuracy: 0.7917 - lr: 1.0000e-04\n",
            "Epoch 453/500\n",
            "98/98 [==============================] - 31s 310ms/step - loss: 0.4788 - accuracy: 0.8778 - val_loss: 0.8918 - val_accuracy: 0.8006 - lr: 1.0000e-04\n",
            "Epoch 454/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.4789 - accuracy: 0.8773 - val_loss: 0.9032 - val_accuracy: 0.8002 - lr: 1.0000e-04\n",
            "Epoch 455/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4798 - accuracy: 0.8774 - val_loss: 0.9265 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
            "Epoch 456/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4796 - accuracy: 0.8775 - val_loss: 0.9292 - val_accuracy: 0.7972 - lr: 1.0000e-04\n",
            "Epoch 457/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 0.4790 - accuracy: 0.8782 - val_loss: 0.9544 - val_accuracy: 0.7942 - lr: 1.0000e-04\n",
            "Epoch 458/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 0.4780 - accuracy: 0.8774 - val_loss: 0.8932 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 459/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4770 - accuracy: 0.8786 - val_loss: 0.9043 - val_accuracy: 0.8003 - lr: 1.0000e-04\n",
            "Epoch 460/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4775 - accuracy: 0.8777 - val_loss: 0.8701 - val_accuracy: 0.7977 - lr: 1.0000e-04\n",
            "Epoch 461/500\n",
            "98/98 [==============================] - 31s 316ms/step - loss: 0.4782 - accuracy: 0.8770 - val_loss: 0.9434 - val_accuracy: 0.7982 - lr: 1.0000e-04\n",
            "Epoch 462/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.4797 - accuracy: 0.8765 - val_loss: 0.9201 - val_accuracy: 0.7984 - lr: 1.0000e-04\n",
            "Epoch 463/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4786 - accuracy: 0.8775 - val_loss: 0.9036 - val_accuracy: 0.7946 - lr: 1.0000e-04\n",
            "Epoch 464/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4799 - accuracy: 0.8774 - val_loss: 0.9183 - val_accuracy: 0.7976 - lr: 1.0000e-04\n",
            "Epoch 465/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 0.4788 - accuracy: 0.8777 - val_loss: 0.9040 - val_accuracy: 0.7994 - lr: 1.0000e-04\n",
            "Epoch 466/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.4786 - accuracy: 0.8773 - val_loss: 0.9121 - val_accuracy: 0.7939 - lr: 1.0000e-04\n",
            "Epoch 467/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4789 - accuracy: 0.8778 - val_loss: 0.9188 - val_accuracy: 0.7947 - lr: 1.0000e-04\n",
            "Epoch 468/500\n",
            "98/98 [==============================] - 30s 306ms/step - loss: 0.4803 - accuracy: 0.8774 - val_loss: 0.8990 - val_accuracy: 0.7976 - lr: 1.0000e-04\n",
            "Epoch 469/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4776 - accuracy: 0.8768 - val_loss: 0.9095 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
            "Epoch 470/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4763 - accuracy: 0.8786 - val_loss: 0.9065 - val_accuracy: 0.7929 - lr: 1.0000e-04\n",
            "Epoch 471/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.4757 - accuracy: 0.8785 - val_loss: 0.9292 - val_accuracy: 0.7919 - lr: 1.0000e-04\n",
            "Epoch 472/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4749 - accuracy: 0.8782 - val_loss: 0.9099 - val_accuracy: 0.7970 - lr: 1.0000e-04\n",
            "Epoch 473/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 0.4762 - accuracy: 0.8787 - val_loss: 0.9118 - val_accuracy: 0.7972 - lr: 1.0000e-04\n",
            "Epoch 474/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.4777 - accuracy: 0.8775 - val_loss: 0.8908 - val_accuracy: 0.7934 - lr: 1.0000e-04\n",
            "Epoch 475/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4765 - accuracy: 0.8776 - val_loss: 0.9103 - val_accuracy: 0.7952 - lr: 1.0000e-04\n",
            "Epoch 476/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4792 - accuracy: 0.8777 - val_loss: 0.8780 - val_accuracy: 0.7972 - lr: 1.0000e-04\n",
            "Epoch 477/500\n",
            "98/98 [==============================] - 30s 300ms/step - loss: 0.4753 - accuracy: 0.8781 - val_loss: 0.9083 - val_accuracy: 0.8009 - lr: 1.0000e-04\n",
            "Epoch 478/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 0.4788 - accuracy: 0.8784 - val_loss: 0.8451 - val_accuracy: 0.7978 - lr: 1.0000e-04\n",
            "Epoch 479/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 0.4764 - accuracy: 0.8780 - val_loss: 0.9050 - val_accuracy: 0.7965 - lr: 1.0000e-04\n",
            "Epoch 480/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 0.4768 - accuracy: 0.8787 - val_loss: 0.9162 - val_accuracy: 0.7992 - lr: 1.0000e-04\n",
            "Epoch 481/500\n",
            "98/98 [==============================] - 31s 317ms/step - loss: 0.4767 - accuracy: 0.8783 - val_loss: 0.8949 - val_accuracy: 0.7967 - lr: 1.0000e-04\n",
            "Epoch 482/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.4760 - accuracy: 0.8784 - val_loss: 0.9206 - val_accuracy: 0.8010 - lr: 1.0000e-04\n",
            "Epoch 483/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 0.4744 - accuracy: 0.8784 - val_loss: 0.9031 - val_accuracy: 0.8005 - lr: 1.0000e-04\n",
            "Epoch 484/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 0.4739 - accuracy: 0.8790 - val_loss: 0.9031 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 485/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.4757 - accuracy: 0.8783 - val_loss: 0.9173 - val_accuracy: 0.7938 - lr: 1.0000e-04\n",
            "Epoch 486/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4744 - accuracy: 0.8782 - val_loss: 0.9175 - val_accuracy: 0.7942 - lr: 1.0000e-04\n",
            "Epoch 487/500\n",
            "98/98 [==============================] - 31s 312ms/step - loss: 0.4754 - accuracy: 0.8783 - val_loss: 0.9354 - val_accuracy: 0.7954 - lr: 1.0000e-04\n",
            "Epoch 488/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 0.4785 - accuracy: 0.8779 - val_loss: 0.8925 - val_accuracy: 0.7999 - lr: 1.0000e-04\n",
            "Epoch 489/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4770 - accuracy: 0.8784 - val_loss: 0.9109 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
            "Epoch 490/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4750 - accuracy: 0.8793 - val_loss: 0.9019 - val_accuracy: 0.7948 - lr: 1.0000e-04\n",
            "Epoch 491/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 0.4755 - accuracy: 0.8785 - val_loss: 0.8741 - val_accuracy: 0.7965 - lr: 1.0000e-04\n",
            "Epoch 492/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4721 - accuracy: 0.8789 - val_loss: 0.9380 - val_accuracy: 0.7962 - lr: 1.0000e-04\n",
            "Epoch 493/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.4763 - accuracy: 0.8771 - val_loss: 0.9071 - val_accuracy: 0.7970 - lr: 1.0000e-04\n",
            "Epoch 494/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 0.4745 - accuracy: 0.8787 - val_loss: 0.9415 - val_accuracy: 0.7899 - lr: 1.0000e-04\n",
            "Epoch 495/500\n",
            "98/98 [==============================] - 31s 311ms/step - loss: 0.4778 - accuracy: 0.8781 - val_loss: 0.9091 - val_accuracy: 0.8006 - lr: 1.0000e-04\n",
            "Epoch 496/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 0.4735 - accuracy: 0.8787 - val_loss: 0.9014 - val_accuracy: 0.7959 - lr: 1.0000e-04\n",
            "Epoch 497/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 0.4740 - accuracy: 0.8791 - val_loss: 0.9252 - val_accuracy: 0.7971 - lr: 1.0000e-04\n",
            "Epoch 498/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 0.4753 - accuracy: 0.8784 - val_loss: 0.8884 - val_accuracy: 0.7997 - lr: 1.0000e-04\n",
            "Epoch 499/500\n",
            "98/98 [==============================] - 31s 315ms/step - loss: 0.4749 - accuracy: 0.8785 - val_loss: 0.8890 - val_accuracy: 0.7982 - lr: 1.0000e-04\n",
            "Epoch 500/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 0.4737 - accuracy: 0.8783 - val_loss: 0.8637 - val_accuracy: 0.8032 - lr: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "model = build_model()\n",
        "print(model.summary())\n",
        "import os\n",
        "# set callback\n",
        "save_dir = '/content/drive/MyDrive/ECE6930/NiN-0207-01/new7'\n",
        "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "checkpointer = keras.callbacks.ModelCheckpoint(os.path.join(save_dir, '{epoch:03d}.h5'), monitor='val_loss', verbose=0,\t\t\t\t\t\tsave_best_only=False, \n",
        " \t\t\t\t\t\t\t\tsave_weights_only=False, mode='auto', \n",
        " \t\t\t\t\t\t\t\tperiod=10)\n",
        "\n",
        "\n",
        "\n",
        "# set data augmentation\n",
        "print('Using real-time data augmentation.')\n",
        "datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# start training\n",
        "history = model.fit(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                    #steps_per_epoch=iterations,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[checkpointer,change_lr],\n",
        "                    validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eaU6yE9WqLtp"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "model.save('nin.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gogqg1HbUf7U",
        "outputId": "88cbb6ab-b1ad-4d6c-dcfa-1dd30b9cc101"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\",\"test\"],loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}