{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxJ8BOvDsrTO",
        "outputId": "1d5f1e75-d9c9-4674-854a-e342012a7605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWkTClo3o42v"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.initializers import RandomNormal  \n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
        "from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n",
        "\n",
        "weight_decay  = 0.0001# 新增\n",
        "batch_size    = 512\n",
        "epochs        = 500\n",
        "#iterations    = 391\n",
        "num_classes   = 100\n",
        "dropout       = 0.2\n",
        "log_filepath  = '/content/drive/MyDrive/ECE6930/NiN-0207-02/new1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXHG6-sdqA1a"
      },
      "outputs": [],
      "source": [
        "def color_preprocessing(x_train,x_test):\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    mean = [125.307, 122.95, 113.865]\n",
        "    std  = [62.9932, 62.0887, 66.7048]\n",
        "    for i in range(3):\n",
        "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
        "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
        "    return x_train, x_test\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch <= 80:\n",
        "        return 0.001\n",
        "    if epoch <= 140:\n",
        "        return 0.0005\n",
        "    return 0.0001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv_AWVkpqElf"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), \n",
        "                     input_shape=x_train.shape[1:]))# 32, 32, 3\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
        "\n",
        "    model.add(Dropout(dropout))\n",
        "    \n",
        "    model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
        "\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(100, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3F7DtdbqGiP",
        "outputId": "4a367d4d-ee34-4791-8a5d-5980516dd05d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 80s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes) # one-hot 编码\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes) # one-hot 编码\n",
        "x_train, x_test = color_preprocessing(x_train, x_test) # 把减均值，除以标准差封装成了函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x7ZQK06r9-T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGxuElYaqJz9",
        "outputId": "d06cd437-efb2-4c8a-fe42-3d932655f0bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 192)       14592     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 192)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 160)       30880     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 160)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 96)        15456     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 32, 32, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 96)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 192)       460992    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 192)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 192)       37056     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16, 16, 192)       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 16, 192)       37056     \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16, 16, 192)       0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 192)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 192)         331968    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 192)         37056     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 100)         19300     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8, 8, 100)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 100)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 100)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 984,356\n",
            "Trainable params: 984,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/500\n",
            "98/98 [==============================] - 44s 306ms/step - loss: 4.5738 - accuracy: 0.0301 - val_loss: 4.4149 - val_accuracy: 0.0592\n",
            "Epoch 2/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 4.2926 - accuracy: 0.0752 - val_loss: 4.1874 - val_accuracy: 0.0891\n",
            "Epoch 3/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 4.0895 - accuracy: 0.1105 - val_loss: 3.9972 - val_accuracy: 0.1280\n",
            "Epoch 4/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 3.9435 - accuracy: 0.1361 - val_loss: 3.8265 - val_accuracy: 0.1527\n",
            "Epoch 5/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 3.7988 - accuracy: 0.1558 - val_loss: 3.7228 - val_accuracy: 0.1733\n",
            "Epoch 6/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 3.7014 - accuracy: 0.1746 - val_loss: 3.6533 - val_accuracy: 0.1862\n",
            "Epoch 7/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 3.5844 - accuracy: 0.1942 - val_loss: 3.5546 - val_accuracy: 0.1990\n",
            "Epoch 8/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 3.4844 - accuracy: 0.2157 - val_loss: 3.4458 - val_accuracy: 0.2246\n",
            "Epoch 9/500\n",
            "98/98 [==============================] - 31s 313ms/step - loss: 3.3870 - accuracy: 0.2337 - val_loss: 3.3354 - val_accuracy: 0.2488\n",
            "Epoch 10/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 3.3060 - accuracy: 0.2547 - val_loss: 3.2698 - val_accuracy: 0.2635\n",
            "Epoch 11/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 3.2372 - accuracy: 0.2685 - val_loss: 3.1787 - val_accuracy: 0.2827\n",
            "Epoch 12/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 3.1705 - accuracy: 0.2857 - val_loss: 3.0889 - val_accuracy: 0.3058\n",
            "Epoch 13/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 3.1074 - accuracy: 0.2971 - val_loss: 3.1499 - val_accuracy: 0.3019\n",
            "Epoch 14/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 3.0574 - accuracy: 0.3120 - val_loss: 3.0289 - val_accuracy: 0.3187\n",
            "Epoch 15/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 2.9959 - accuracy: 0.3244 - val_loss: 3.0360 - val_accuracy: 0.3301\n",
            "Epoch 16/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 2.9428 - accuracy: 0.3363 - val_loss: 2.9586 - val_accuracy: 0.3413\n",
            "Epoch 17/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 2.8920 - accuracy: 0.3481 - val_loss: 2.9297 - val_accuracy: 0.3458\n",
            "Epoch 18/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 2.8506 - accuracy: 0.3571 - val_loss: 2.8368 - val_accuracy: 0.3637\n",
            "Epoch 19/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 2.8044 - accuracy: 0.3652 - val_loss: 2.9052 - val_accuracy: 0.3549\n",
            "Epoch 20/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 2.7848 - accuracy: 0.3733 - val_loss: 2.8187 - val_accuracy: 0.3764\n",
            "Epoch 21/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.7444 - accuracy: 0.3827 - val_loss: 2.8711 - val_accuracy: 0.3622\n",
            "Epoch 22/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 2.7099 - accuracy: 0.3894 - val_loss: 2.8040 - val_accuracy: 0.3855\n",
            "Epoch 23/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 2.6739 - accuracy: 0.4005 - val_loss: 2.7638 - val_accuracy: 0.3903\n",
            "Epoch 24/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 2.6398 - accuracy: 0.4061 - val_loss: 2.7975 - val_accuracy: 0.3850\n",
            "Epoch 25/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.6135 - accuracy: 0.4130 - val_loss: 2.7862 - val_accuracy: 0.3897\n",
            "Epoch 26/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 2.5790 - accuracy: 0.4211 - val_loss: 2.7708 - val_accuracy: 0.3943\n",
            "Epoch 27/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.5486 - accuracy: 0.4284 - val_loss: 2.6961 - val_accuracy: 0.4090\n",
            "Epoch 28/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 2.5430 - accuracy: 0.4332 - val_loss: 2.6260 - val_accuracy: 0.4215\n",
            "Epoch 29/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 2.5067 - accuracy: 0.4398 - val_loss: 2.6065 - val_accuracy: 0.4221\n",
            "Epoch 30/500\n",
            "98/98 [==============================] - 30s 310ms/step - loss: 2.4781 - accuracy: 0.4483 - val_loss: 2.6858 - val_accuracy: 0.4086\n",
            "Epoch 31/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 2.4715 - accuracy: 0.4509 - val_loss: 2.5765 - val_accuracy: 0.4384\n",
            "Epoch 32/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.4450 - accuracy: 0.4562 - val_loss: 2.5877 - val_accuracy: 0.4346\n",
            "Epoch 33/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.4289 - accuracy: 0.4610 - val_loss: 2.5436 - val_accuracy: 0.4411\n",
            "Epoch 34/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 2.4031 - accuracy: 0.4651 - val_loss: 2.5608 - val_accuracy: 0.4398\n",
            "Epoch 35/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 2.3667 - accuracy: 0.4760 - val_loss: 2.5828 - val_accuracy: 0.4340\n",
            "Epoch 36/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 2.3708 - accuracy: 0.4751 - val_loss: 2.5056 - val_accuracy: 0.4483\n",
            "Epoch 37/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 2.3471 - accuracy: 0.4796 - val_loss: 2.4862 - val_accuracy: 0.4591\n",
            "Epoch 38/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 2.3237 - accuracy: 0.4875 - val_loss: 2.5147 - val_accuracy: 0.4466\n",
            "Epoch 39/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 2.3115 - accuracy: 0.4895 - val_loss: 2.4797 - val_accuracy: 0.4576\n",
            "Epoch 40/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 2.2786 - accuracy: 0.4989 - val_loss: 2.5571 - val_accuracy: 0.4500\n",
            "Epoch 41/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.2711 - accuracy: 0.5026 - val_loss: 2.5814 - val_accuracy: 0.4452\n",
            "Epoch 42/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.2699 - accuracy: 0.5022 - val_loss: 2.4390 - val_accuracy: 0.4742\n",
            "Epoch 43/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 2.2542 - accuracy: 0.5029 - val_loss: 2.4636 - val_accuracy: 0.4667\n",
            "Epoch 44/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 2.2388 - accuracy: 0.5091 - val_loss: 2.5554 - val_accuracy: 0.4549\n",
            "Epoch 45/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 2.2078 - accuracy: 0.5166 - val_loss: 2.5249 - val_accuracy: 0.4599\n",
            "Epoch 46/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 2.1932 - accuracy: 0.5228 - val_loss: 2.5056 - val_accuracy: 0.4758\n",
            "Epoch 47/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 2.1864 - accuracy: 0.5233 - val_loss: 2.4729 - val_accuracy: 0.4753\n",
            "Epoch 48/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 2.1684 - accuracy: 0.5303 - val_loss: 2.5080 - val_accuracy: 0.4636\n",
            "Epoch 49/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.1674 - accuracy: 0.5281 - val_loss: 2.4772 - val_accuracy: 0.4777\n",
            "Epoch 50/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 2.1500 - accuracy: 0.5341 - val_loss: 2.4268 - val_accuracy: 0.4797\n",
            "Epoch 51/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 2.1357 - accuracy: 0.5361 - val_loss: 2.5265 - val_accuracy: 0.4702\n",
            "Epoch 52/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 2.1268 - accuracy: 0.5382 - val_loss: 2.5485 - val_accuracy: 0.4783\n",
            "Epoch 53/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 2.1191 - accuracy: 0.5402 - val_loss: 2.5115 - val_accuracy: 0.4730\n",
            "Epoch 54/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.0996 - accuracy: 0.5478 - val_loss: 2.4633 - val_accuracy: 0.4841\n",
            "Epoch 55/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 2.1054 - accuracy: 0.5451 - val_loss: 2.3740 - val_accuracy: 0.4934\n",
            "Epoch 56/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 2.0778 - accuracy: 0.5523 - val_loss: 2.4231 - val_accuracy: 0.4897\n",
            "Epoch 57/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 2.0737 - accuracy: 0.5540 - val_loss: 2.4460 - val_accuracy: 0.4887\n",
            "Epoch 58/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 2.0637 - accuracy: 0.5571 - val_loss: 2.3973 - val_accuracy: 0.4971\n",
            "Epoch 59/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 2.0651 - accuracy: 0.5571 - val_loss: 2.3994 - val_accuracy: 0.4934\n",
            "Epoch 60/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 2.0434 - accuracy: 0.5637 - val_loss: 2.3877 - val_accuracy: 0.4949\n",
            "Epoch 61/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 2.0360 - accuracy: 0.5656 - val_loss: 2.4110 - val_accuracy: 0.4971\n",
            "Epoch 62/500\n",
            "98/98 [==============================] - 30s 307ms/step - loss: 2.0292 - accuracy: 0.5685 - val_loss: 2.3985 - val_accuracy: 0.4966\n",
            "Epoch 63/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 2.0141 - accuracy: 0.5715 - val_loss: 2.4199 - val_accuracy: 0.4982\n",
            "Epoch 64/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 2.0091 - accuracy: 0.5720 - val_loss: 2.4252 - val_accuracy: 0.4953\n",
            "Epoch 65/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.9874 - accuracy: 0.5787 - val_loss: 2.4019 - val_accuracy: 0.4981\n",
            "Epoch 66/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.9943 - accuracy: 0.5759 - val_loss: 2.3473 - val_accuracy: 0.5070\n",
            "Epoch 67/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 1.9881 - accuracy: 0.5786 - val_loss: 2.4348 - val_accuracy: 0.5028\n",
            "Epoch 68/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.9728 - accuracy: 0.5835 - val_loss: 2.4741 - val_accuracy: 0.4979\n",
            "Epoch 69/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.9696 - accuracy: 0.5858 - val_loss: 2.4292 - val_accuracy: 0.5088\n",
            "Epoch 70/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.9507 - accuracy: 0.5916 - val_loss: 2.4158 - val_accuracy: 0.5028\n",
            "Epoch 71/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.9432 - accuracy: 0.5920 - val_loss: 2.5257 - val_accuracy: 0.4979\n",
            "Epoch 72/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.9453 - accuracy: 0.5909 - val_loss: 2.4720 - val_accuracy: 0.5039\n",
            "Epoch 73/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.9392 - accuracy: 0.5931 - val_loss: 2.5180 - val_accuracy: 0.5070\n",
            "Epoch 74/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 1.9129 - accuracy: 0.6011 - val_loss: 2.4436 - val_accuracy: 0.4998\n",
            "Epoch 75/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.9106 - accuracy: 0.6025 - val_loss: 2.3489 - val_accuracy: 0.5074\n",
            "Epoch 76/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.9185 - accuracy: 0.5971 - val_loss: 2.3820 - val_accuracy: 0.5066\n",
            "Epoch 77/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.8957 - accuracy: 0.6046 - val_loss: 2.4828 - val_accuracy: 0.4981\n",
            "Epoch 78/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.8987 - accuracy: 0.6042 - val_loss: 2.3917 - val_accuracy: 0.5133\n",
            "Epoch 79/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.8868 - accuracy: 0.6075 - val_loss: 2.4699 - val_accuracy: 0.5076\n",
            "Epoch 80/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 1.8853 - accuracy: 0.6082 - val_loss: 2.4083 - val_accuracy: 0.5169\n",
            "Epoch 81/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.8695 - accuracy: 0.6133 - val_loss: 2.4095 - val_accuracy: 0.5190\n",
            "Epoch 82/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.8665 - accuracy: 0.6133 - val_loss: 2.3962 - val_accuracy: 0.5176\n",
            "Epoch 83/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 1.8512 - accuracy: 0.6195 - val_loss: 2.4939 - val_accuracy: 0.5040\n",
            "Epoch 84/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.8626 - accuracy: 0.6147 - val_loss: 2.3802 - val_accuracy: 0.5195\n",
            "Epoch 85/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.8560 - accuracy: 0.6173 - val_loss: 2.4781 - val_accuracy: 0.5118\n",
            "Epoch 86/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.8364 - accuracy: 0.6239 - val_loss: 2.4121 - val_accuracy: 0.5173\n",
            "Epoch 87/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.8372 - accuracy: 0.6221 - val_loss: 2.4364 - val_accuracy: 0.5196\n",
            "Epoch 88/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.8337 - accuracy: 0.6231 - val_loss: 2.4491 - val_accuracy: 0.5230\n",
            "Epoch 89/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.8276 - accuracy: 0.6237 - val_loss: 2.4091 - val_accuracy: 0.5247\n",
            "Epoch 90/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.8298 - accuracy: 0.6245 - val_loss: 2.3799 - val_accuracy: 0.5241\n",
            "Epoch 91/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.8040 - accuracy: 0.6304 - val_loss: 2.3836 - val_accuracy: 0.5275\n",
            "Epoch 92/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.7973 - accuracy: 0.6331 - val_loss: 2.3978 - val_accuracy: 0.5171\n",
            "Epoch 93/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.8078 - accuracy: 0.6296 - val_loss: 2.4487 - val_accuracy: 0.5112\n",
            "Epoch 94/500\n",
            "98/98 [==============================] - 30s 309ms/step - loss: 1.8134 - accuracy: 0.6287 - val_loss: 2.4178 - val_accuracy: 0.5181\n",
            "Epoch 95/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.7830 - accuracy: 0.6398 - val_loss: 2.4083 - val_accuracy: 0.5170\n",
            "Epoch 96/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.7856 - accuracy: 0.6382 - val_loss: 2.3717 - val_accuracy: 0.5312\n",
            "Epoch 97/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.7761 - accuracy: 0.6394 - val_loss: 2.4421 - val_accuracy: 0.5250\n",
            "Epoch 98/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.7887 - accuracy: 0.6362 - val_loss: 2.4342 - val_accuracy: 0.5249\n",
            "Epoch 99/500\n",
            "98/98 [==============================] - 28s 291ms/step - loss: 1.7695 - accuracy: 0.6415 - val_loss: 2.3724 - val_accuracy: 0.5385\n",
            "Epoch 100/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.7621 - accuracy: 0.6443 - val_loss: 2.4325 - val_accuracy: 0.5193\n",
            "Epoch 101/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.7618 - accuracy: 0.6444 - val_loss: 2.4742 - val_accuracy: 0.5281\n",
            "Epoch 102/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.7623 - accuracy: 0.6436 - val_loss: 2.3926 - val_accuracy: 0.5382\n",
            "Epoch 103/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.7456 - accuracy: 0.6472 - val_loss: 2.5992 - val_accuracy: 0.5201\n",
            "Epoch 104/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.7489 - accuracy: 0.6473 - val_loss: 2.4642 - val_accuracy: 0.5298\n",
            "Epoch 105/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 1.7502 - accuracy: 0.6466 - val_loss: 2.5599 - val_accuracy: 0.5221\n",
            "Epoch 106/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.7409 - accuracy: 0.6515 - val_loss: 2.4462 - val_accuracy: 0.5201\n",
            "Epoch 107/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.7411 - accuracy: 0.6524 - val_loss: 2.4073 - val_accuracy: 0.5211\n",
            "Epoch 108/500\n",
            "98/98 [==============================] - 30s 301ms/step - loss: 1.7319 - accuracy: 0.6529 - val_loss: 2.4740 - val_accuracy: 0.5107\n",
            "Epoch 109/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.7254 - accuracy: 0.6550 - val_loss: 2.4834 - val_accuracy: 0.5192\n",
            "Epoch 110/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.7189 - accuracy: 0.6565 - val_loss: 2.6258 - val_accuracy: 0.5142\n",
            "Epoch 111/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.7197 - accuracy: 0.6563 - val_loss: 2.4546 - val_accuracy: 0.5211\n",
            "Epoch 112/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.7107 - accuracy: 0.6580 - val_loss: 2.4668 - val_accuracy: 0.5326\n",
            "Epoch 113/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.7125 - accuracy: 0.6587 - val_loss: 2.5195 - val_accuracy: 0.5294\n",
            "Epoch 114/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.7065 - accuracy: 0.6605 - val_loss: 2.4713 - val_accuracy: 0.5175\n",
            "Epoch 115/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.7050 - accuracy: 0.6625 - val_loss: 2.4939 - val_accuracy: 0.5197\n",
            "Epoch 116/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.7037 - accuracy: 0.6634 - val_loss: 2.4812 - val_accuracy: 0.5326\n",
            "Epoch 117/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.6917 - accuracy: 0.6670 - val_loss: 2.5163 - val_accuracy: 0.5249\n",
            "Epoch 118/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.6924 - accuracy: 0.6652 - val_loss: 2.4317 - val_accuracy: 0.5332\n",
            "Epoch 119/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.6817 - accuracy: 0.6692 - val_loss: 2.4715 - val_accuracy: 0.5332\n",
            "Epoch 120/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 1.6721 - accuracy: 0.6718 - val_loss: 2.5822 - val_accuracy: 0.5235\n",
            "Epoch 121/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 1.6840 - accuracy: 0.6699 - val_loss: 2.4885 - val_accuracy: 0.5218\n",
            "Epoch 122/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 1.6779 - accuracy: 0.6701 - val_loss: 2.4699 - val_accuracy: 0.5389\n",
            "Epoch 123/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.6688 - accuracy: 0.6725 - val_loss: 2.5077 - val_accuracy: 0.5275\n",
            "Epoch 124/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.6631 - accuracy: 0.6740 - val_loss: 2.4738 - val_accuracy: 0.5320\n",
            "Epoch 125/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.6477 - accuracy: 0.6794 - val_loss: 2.5019 - val_accuracy: 0.5312\n",
            "Epoch 126/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.6683 - accuracy: 0.6718 - val_loss: 2.4505 - val_accuracy: 0.5354\n",
            "Epoch 127/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.6716 - accuracy: 0.6739 - val_loss: 2.4211 - val_accuracy: 0.5436\n",
            "Epoch 128/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 1.6531 - accuracy: 0.6779 - val_loss: 2.4799 - val_accuracy: 0.5388\n",
            "Epoch 129/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.6550 - accuracy: 0.6770 - val_loss: 2.5818 - val_accuracy: 0.5170\n",
            "Epoch 130/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.6423 - accuracy: 0.6794 - val_loss: 2.4668 - val_accuracy: 0.5324\n",
            "Epoch 131/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.6459 - accuracy: 0.6781 - val_loss: 2.5194 - val_accuracy: 0.5326\n",
            "Epoch 132/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.6511 - accuracy: 0.6806 - val_loss: 2.4343 - val_accuracy: 0.5407\n",
            "Epoch 133/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.6407 - accuracy: 0.6795 - val_loss: 2.5102 - val_accuracy: 0.5260\n",
            "Epoch 134/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.6361 - accuracy: 0.6819 - val_loss: 2.5016 - val_accuracy: 0.5355\n",
            "Epoch 135/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.6325 - accuracy: 0.6827 - val_loss: 2.5434 - val_accuracy: 0.5318\n",
            "Epoch 136/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 1.6316 - accuracy: 0.6851 - val_loss: 2.6257 - val_accuracy: 0.5251\n",
            "Epoch 137/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.6250 - accuracy: 0.6858 - val_loss: 2.4415 - val_accuracy: 0.5365\n",
            "Epoch 138/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 1.6242 - accuracy: 0.6876 - val_loss: 2.5342 - val_accuracy: 0.5257\n",
            "Epoch 139/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.6244 - accuracy: 0.6853 - val_loss: 2.4780 - val_accuracy: 0.5304\n",
            "Epoch 140/500\n",
            "98/98 [==============================] - 29s 299ms/step - loss: 1.6247 - accuracy: 0.6872 - val_loss: 2.5259 - val_accuracy: 0.5289\n",
            "Epoch 141/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.6171 - accuracy: 0.6904 - val_loss: 2.5791 - val_accuracy: 0.5184\n",
            "Epoch 142/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.6032 - accuracy: 0.6946 - val_loss: 2.4830 - val_accuracy: 0.5320\n",
            "Epoch 143/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.6062 - accuracy: 0.6907 - val_loss: 2.4744 - val_accuracy: 0.5433\n",
            "Epoch 144/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.5981 - accuracy: 0.6939 - val_loss: 2.5963 - val_accuracy: 0.5265\n",
            "Epoch 145/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.5950 - accuracy: 0.6955 - val_loss: 2.4725 - val_accuracy: 0.5399\n",
            "Epoch 146/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.6105 - accuracy: 0.6919 - val_loss: 2.4695 - val_accuracy: 0.5354\n",
            "Epoch 147/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.6037 - accuracy: 0.6929 - val_loss: 2.5011 - val_accuracy: 0.5429\n",
            "Epoch 148/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.6011 - accuracy: 0.6931 - val_loss: 2.5894 - val_accuracy: 0.5283\n",
            "Epoch 149/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.6004 - accuracy: 0.6937 - val_loss: 2.6298 - val_accuracy: 0.5338\n",
            "Epoch 150/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.5858 - accuracy: 0.6995 - val_loss: 2.5600 - val_accuracy: 0.5320\n",
            "Epoch 151/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.5909 - accuracy: 0.6974 - val_loss: 2.5092 - val_accuracy: 0.5280\n",
            "Epoch 152/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.5963 - accuracy: 0.6959 - val_loss: 2.6276 - val_accuracy: 0.5264\n",
            "Epoch 153/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.5854 - accuracy: 0.6970 - val_loss: 2.5079 - val_accuracy: 0.5427\n",
            "Epoch 154/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.5874 - accuracy: 0.6966 - val_loss: 2.5925 - val_accuracy: 0.5333\n",
            "Epoch 155/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.5822 - accuracy: 0.7016 - val_loss: 2.4882 - val_accuracy: 0.5355\n",
            "Epoch 156/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.5772 - accuracy: 0.7005 - val_loss: 2.5379 - val_accuracy: 0.5353\n",
            "Epoch 157/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.5731 - accuracy: 0.7038 - val_loss: 2.4960 - val_accuracy: 0.5408\n",
            "Epoch 158/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 1.5762 - accuracy: 0.7027 - val_loss: 2.5030 - val_accuracy: 0.5403\n",
            "Epoch 159/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.5746 - accuracy: 0.7003 - val_loss: 2.4860 - val_accuracy: 0.5393\n",
            "Epoch 160/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.5542 - accuracy: 0.7080 - val_loss: 2.5258 - val_accuracy: 0.5356\n",
            "Epoch 161/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.5590 - accuracy: 0.7073 - val_loss: 2.5471 - val_accuracy: 0.5387\n",
            "Epoch 162/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.5526 - accuracy: 0.7083 - val_loss: 2.5931 - val_accuracy: 0.5299\n",
            "Epoch 163/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.5682 - accuracy: 0.7051 - val_loss: 2.6264 - val_accuracy: 0.5279\n",
            "Epoch 164/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.5546 - accuracy: 0.7074 - val_loss: 2.5995 - val_accuracy: 0.5293\n",
            "Epoch 165/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.5554 - accuracy: 0.7106 - val_loss: 2.5298 - val_accuracy: 0.5403\n",
            "Epoch 166/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.5585 - accuracy: 0.7077 - val_loss: 2.6721 - val_accuracy: 0.5316\n",
            "Epoch 167/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.5459 - accuracy: 0.7119 - val_loss: 2.7122 - val_accuracy: 0.5264\n",
            "Epoch 168/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.5507 - accuracy: 0.7101 - val_loss: 2.5284 - val_accuracy: 0.5340\n",
            "Epoch 169/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.5459 - accuracy: 0.7120 - val_loss: 2.5586 - val_accuracy: 0.5389\n",
            "Epoch 170/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.5426 - accuracy: 0.7117 - val_loss: 2.5477 - val_accuracy: 0.5382\n",
            "Epoch 171/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.5397 - accuracy: 0.7126 - val_loss: 2.5725 - val_accuracy: 0.5433\n",
            "Epoch 172/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.5361 - accuracy: 0.7139 - val_loss: 2.6342 - val_accuracy: 0.5437\n",
            "Epoch 173/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.5361 - accuracy: 0.7139 - val_loss: 2.5540 - val_accuracy: 0.5361\n",
            "Epoch 174/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.5436 - accuracy: 0.7127 - val_loss: 2.5368 - val_accuracy: 0.5292\n",
            "Epoch 175/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.5338 - accuracy: 0.7137 - val_loss: 2.5204 - val_accuracy: 0.5388\n",
            "Epoch 176/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.5402 - accuracy: 0.7153 - val_loss: 2.6315 - val_accuracy: 0.5398\n",
            "Epoch 177/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.5177 - accuracy: 0.7209 - val_loss: 2.6591 - val_accuracy: 0.5268\n",
            "Epoch 178/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.5297 - accuracy: 0.7164 - val_loss: 2.5386 - val_accuracy: 0.5345\n",
            "Epoch 179/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.5227 - accuracy: 0.7173 - val_loss: 2.6504 - val_accuracy: 0.5366\n",
            "Epoch 180/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.5454 - accuracy: 0.7128 - val_loss: 2.6631 - val_accuracy: 0.5340\n",
            "Epoch 181/500\n",
            "98/98 [==============================] - 30s 302ms/step - loss: 1.5130 - accuracy: 0.7224 - val_loss: 2.6693 - val_accuracy: 0.5248\n",
            "Epoch 182/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.5279 - accuracy: 0.7168 - val_loss: 2.6935 - val_accuracy: 0.5375\n",
            "Epoch 183/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.5239 - accuracy: 0.7168 - val_loss: 2.6466 - val_accuracy: 0.5315\n",
            "Epoch 184/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.5100 - accuracy: 0.7228 - val_loss: 2.5346 - val_accuracy: 0.5406\n",
            "Epoch 185/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.5077 - accuracy: 0.7239 - val_loss: 2.6063 - val_accuracy: 0.5301\n",
            "Epoch 186/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.5225 - accuracy: 0.7187 - val_loss: 2.5836 - val_accuracy: 0.5230\n",
            "Epoch 187/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.5183 - accuracy: 0.7195 - val_loss: 2.5868 - val_accuracy: 0.5316\n",
            "Epoch 188/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.5077 - accuracy: 0.7243 - val_loss: 2.6956 - val_accuracy: 0.5392\n",
            "Epoch 189/500\n",
            "98/98 [==============================] - 30s 305ms/step - loss: 1.5151 - accuracy: 0.7229 - val_loss: 2.6453 - val_accuracy: 0.5389\n",
            "Epoch 190/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.5082 - accuracy: 0.7236 - val_loss: 2.5700 - val_accuracy: 0.5369\n",
            "Epoch 191/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.5014 - accuracy: 0.7258 - val_loss: 2.6570 - val_accuracy: 0.5341\n",
            "Epoch 192/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.5127 - accuracy: 0.7212 - val_loss: 2.6353 - val_accuracy: 0.5405\n",
            "Epoch 193/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.5066 - accuracy: 0.7225 - val_loss: 2.6382 - val_accuracy: 0.5397\n",
            "Epoch 194/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4959 - accuracy: 0.7270 - val_loss: 2.7416 - val_accuracy: 0.5187\n",
            "Epoch 195/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.5069 - accuracy: 0.7257 - val_loss: 2.6250 - val_accuracy: 0.5361\n",
            "Epoch 196/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.5084 - accuracy: 0.7245 - val_loss: 2.6398 - val_accuracy: 0.5319\n",
            "Epoch 197/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4828 - accuracy: 0.7304 - val_loss: 2.5703 - val_accuracy: 0.5318\n",
            "Epoch 198/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4926 - accuracy: 0.7280 - val_loss: 2.7253 - val_accuracy: 0.5309\n",
            "Epoch 199/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4952 - accuracy: 0.7269 - val_loss: 2.5554 - val_accuracy: 0.5443\n",
            "Epoch 200/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.4942 - accuracy: 0.7292 - val_loss: 2.6459 - val_accuracy: 0.5410\n",
            "Epoch 201/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4966 - accuracy: 0.7287 - val_loss: 2.5206 - val_accuracy: 0.5416\n",
            "Epoch 202/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.4848 - accuracy: 0.7323 - val_loss: 2.6554 - val_accuracy: 0.5301\n",
            "Epoch 203/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.5012 - accuracy: 0.7252 - val_loss: 2.6731 - val_accuracy: 0.5397\n",
            "Epoch 204/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4832 - accuracy: 0.7335 - val_loss: 2.6322 - val_accuracy: 0.5481\n",
            "Epoch 205/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.4979 - accuracy: 0.7285 - val_loss: 2.6483 - val_accuracy: 0.5379\n",
            "Epoch 206/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4723 - accuracy: 0.7320 - val_loss: 2.6104 - val_accuracy: 0.5459\n",
            "Epoch 207/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4917 - accuracy: 0.7290 - val_loss: 2.6956 - val_accuracy: 0.5356\n",
            "Epoch 208/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.4889 - accuracy: 0.7299 - val_loss: 2.6947 - val_accuracy: 0.5458\n",
            "Epoch 209/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4802 - accuracy: 0.7330 - val_loss: 2.6225 - val_accuracy: 0.5353\n",
            "Epoch 210/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4839 - accuracy: 0.7323 - val_loss: 2.6883 - val_accuracy: 0.5441\n",
            "Epoch 211/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.4771 - accuracy: 0.7328 - val_loss: 2.6607 - val_accuracy: 0.5430\n",
            "Epoch 212/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4822 - accuracy: 0.7318 - val_loss: 2.6511 - val_accuracy: 0.5329\n",
            "Epoch 213/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 1.4717 - accuracy: 0.7352 - val_loss: 2.5826 - val_accuracy: 0.5432\n",
            "Epoch 214/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.4736 - accuracy: 0.7336 - val_loss: 2.7081 - val_accuracy: 0.5364\n",
            "Epoch 215/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.4784 - accuracy: 0.7322 - val_loss: 2.7282 - val_accuracy: 0.5346\n",
            "Epoch 216/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4710 - accuracy: 0.7335 - val_loss: 2.6751 - val_accuracy: 0.5427\n",
            "Epoch 217/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4675 - accuracy: 0.7353 - val_loss: 2.6167 - val_accuracy: 0.5415\n",
            "Epoch 218/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 1.4688 - accuracy: 0.7357 - val_loss: 2.7124 - val_accuracy: 0.5379\n",
            "Epoch 219/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.4672 - accuracy: 0.7370 - val_loss: 2.6876 - val_accuracy: 0.5364\n",
            "Epoch 220/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 1.4641 - accuracy: 0.7395 - val_loss: 2.7476 - val_accuracy: 0.5270\n",
            "Epoch 221/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.4622 - accuracy: 0.7387 - val_loss: 2.7028 - val_accuracy: 0.5421\n",
            "Epoch 222/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4644 - accuracy: 0.7366 - val_loss: 2.7612 - val_accuracy: 0.5306\n",
            "Epoch 223/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4704 - accuracy: 0.7345 - val_loss: 2.5788 - val_accuracy: 0.5403\n",
            "Epoch 224/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.4600 - accuracy: 0.7398 - val_loss: 2.5530 - val_accuracy: 0.5467\n",
            "Epoch 225/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4558 - accuracy: 0.7401 - val_loss: 2.6501 - val_accuracy: 0.5362\n",
            "Epoch 226/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4592 - accuracy: 0.7390 - val_loss: 2.6442 - val_accuracy: 0.5373\n",
            "Epoch 227/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.4563 - accuracy: 0.7401 - val_loss: 2.6354 - val_accuracy: 0.5355\n",
            "Epoch 228/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4425 - accuracy: 0.7459 - val_loss: 2.6186 - val_accuracy: 0.5335\n",
            "Epoch 229/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4619 - accuracy: 0.7397 - val_loss: 2.6491 - val_accuracy: 0.5408\n",
            "Epoch 230/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.4589 - accuracy: 0.7408 - val_loss: 2.6843 - val_accuracy: 0.5373\n",
            "Epoch 231/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.4557 - accuracy: 0.7425 - val_loss: 2.7437 - val_accuracy: 0.5355\n",
            "Epoch 232/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4431 - accuracy: 0.7428 - val_loss: 2.7249 - val_accuracy: 0.5354\n",
            "Epoch 233/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.4576 - accuracy: 0.7395 - val_loss: 2.6851 - val_accuracy: 0.5356\n",
            "Epoch 234/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4510 - accuracy: 0.7405 - val_loss: 2.5730 - val_accuracy: 0.5398\n",
            "Epoch 235/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.4501 - accuracy: 0.7427 - val_loss: 2.7559 - val_accuracy: 0.5415\n",
            "Epoch 236/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 1.4549 - accuracy: 0.7405 - val_loss: 2.6800 - val_accuracy: 0.5345\n",
            "Epoch 237/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4547 - accuracy: 0.7407 - val_loss: 2.7941 - val_accuracy: 0.5300\n",
            "Epoch 238/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4358 - accuracy: 0.7454 - val_loss: 2.6316 - val_accuracy: 0.5462\n",
            "Epoch 239/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4367 - accuracy: 0.7472 - val_loss: 2.7765 - val_accuracy: 0.5461\n",
            "Epoch 240/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.4442 - accuracy: 0.7445 - val_loss: 2.6259 - val_accuracy: 0.5422\n",
            "Epoch 241/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.4466 - accuracy: 0.7437 - val_loss: 2.7755 - val_accuracy: 0.5367\n",
            "Epoch 242/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 1.4540 - accuracy: 0.7422 - val_loss: 2.7615 - val_accuracy: 0.5377\n",
            "Epoch 243/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 1.4363 - accuracy: 0.7474 - val_loss: 2.7822 - val_accuracy: 0.5360\n",
            "Epoch 244/500\n",
            "98/98 [==============================] - 29s 297ms/step - loss: 1.4501 - accuracy: 0.7432 - val_loss: 2.6323 - val_accuracy: 0.5358\n",
            "Epoch 245/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4562 - accuracy: 0.7418 - val_loss: 2.5945 - val_accuracy: 0.5413\n",
            "Epoch 246/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4308 - accuracy: 0.7484 - val_loss: 2.7686 - val_accuracy: 0.5350\n",
            "Epoch 247/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4467 - accuracy: 0.7439 - val_loss: 2.6451 - val_accuracy: 0.5363\n",
            "Epoch 248/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4219 - accuracy: 0.7508 - val_loss: 2.6388 - val_accuracy: 0.5385\n",
            "Epoch 249/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4362 - accuracy: 0.7483 - val_loss: 2.7366 - val_accuracy: 0.5412\n",
            "Epoch 250/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4371 - accuracy: 0.7474 - val_loss: 2.7060 - val_accuracy: 0.5294\n",
            "Epoch 251/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4429 - accuracy: 0.7461 - val_loss: 2.8282 - val_accuracy: 0.5352\n",
            "Epoch 252/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.4261 - accuracy: 0.7501 - val_loss: 2.8056 - val_accuracy: 0.5374\n",
            "Epoch 253/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4291 - accuracy: 0.7486 - val_loss: 2.6091 - val_accuracy: 0.5381\n",
            "Epoch 254/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4254 - accuracy: 0.7519 - val_loss: 2.6656 - val_accuracy: 0.5348\n",
            "Epoch 255/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4342 - accuracy: 0.7485 - val_loss: 2.7160 - val_accuracy: 0.5351\n",
            "Epoch 256/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4272 - accuracy: 0.7512 - val_loss: 2.7706 - val_accuracy: 0.5324\n",
            "Epoch 257/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.4363 - accuracy: 0.7473 - val_loss: 2.8346 - val_accuracy: 0.5298\n",
            "Epoch 258/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4316 - accuracy: 0.7497 - val_loss: 2.7652 - val_accuracy: 0.5313\n",
            "Epoch 259/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4151 - accuracy: 0.7529 - val_loss: 2.6651 - val_accuracy: 0.5397\n",
            "Epoch 260/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4227 - accuracy: 0.7518 - val_loss: 2.7571 - val_accuracy: 0.5372\n",
            "Epoch 261/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.4230 - accuracy: 0.7514 - val_loss: 2.7211 - val_accuracy: 0.5318\n",
            "Epoch 262/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.4292 - accuracy: 0.7505 - val_loss: 2.6795 - val_accuracy: 0.5336\n",
            "Epoch 263/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4174 - accuracy: 0.7546 - val_loss: 2.6857 - val_accuracy: 0.5411\n",
            "Epoch 264/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4288 - accuracy: 0.7508 - val_loss: 2.7990 - val_accuracy: 0.5376\n",
            "Epoch 265/500\n",
            "98/98 [==============================] - 29s 289ms/step - loss: 1.4170 - accuracy: 0.7544 - val_loss: 2.6749 - val_accuracy: 0.5448\n",
            "Epoch 266/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 1.4212 - accuracy: 0.7534 - val_loss: 2.8250 - val_accuracy: 0.5353\n",
            "Epoch 267/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4240 - accuracy: 0.7503 - val_loss: 2.6187 - val_accuracy: 0.5449\n",
            "Epoch 268/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4144 - accuracy: 0.7539 - val_loss: 2.7521 - val_accuracy: 0.5405\n",
            "Epoch 269/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4214 - accuracy: 0.7522 - val_loss: 2.7414 - val_accuracy: 0.5360\n",
            "Epoch 270/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.4184 - accuracy: 0.7532 - val_loss: 2.6807 - val_accuracy: 0.5442\n",
            "Epoch 271/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.4001 - accuracy: 0.7585 - val_loss: 2.6926 - val_accuracy: 0.5459\n",
            "Epoch 272/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4042 - accuracy: 0.7569 - val_loss: 2.6569 - val_accuracy: 0.5373\n",
            "Epoch 273/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4081 - accuracy: 0.7570 - val_loss: 2.8739 - val_accuracy: 0.5369\n",
            "Epoch 274/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.4240 - accuracy: 0.7530 - val_loss: 2.7891 - val_accuracy: 0.5354\n",
            "Epoch 275/500\n",
            "98/98 [==============================] - 29s 296ms/step - loss: 1.4156 - accuracy: 0.7549 - val_loss: 2.8421 - val_accuracy: 0.5343\n",
            "Epoch 276/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4265 - accuracy: 0.7508 - val_loss: 2.9177 - val_accuracy: 0.5352\n",
            "Epoch 277/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4097 - accuracy: 0.7547 - val_loss: 2.7976 - val_accuracy: 0.5392\n",
            "Epoch 278/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.4008 - accuracy: 0.7580 - val_loss: 2.7537 - val_accuracy: 0.5342\n",
            "Epoch 279/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.4005 - accuracy: 0.7588 - val_loss: 2.7668 - val_accuracy: 0.5399\n",
            "Epoch 280/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.4131 - accuracy: 0.7550 - val_loss: 2.7284 - val_accuracy: 0.5469\n",
            "Epoch 281/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.4125 - accuracy: 0.7555 - val_loss: 2.6549 - val_accuracy: 0.5424\n",
            "Epoch 282/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3955 - accuracy: 0.7614 - val_loss: 2.7740 - val_accuracy: 0.5477\n",
            "Epoch 283/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.4185 - accuracy: 0.7532 - val_loss: 2.7558 - val_accuracy: 0.5422\n",
            "Epoch 284/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.4055 - accuracy: 0.7576 - val_loss: 2.7640 - val_accuracy: 0.5389\n",
            "Epoch 285/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.4017 - accuracy: 0.7591 - val_loss: 2.7261 - val_accuracy: 0.5389\n",
            "Epoch 286/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3992 - accuracy: 0.7580 - val_loss: 2.7166 - val_accuracy: 0.5413\n",
            "Epoch 287/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.4048 - accuracy: 0.7585 - val_loss: 2.7342 - val_accuracy: 0.5450\n",
            "Epoch 288/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.4010 - accuracy: 0.7576 - val_loss: 2.7310 - val_accuracy: 0.5272\n",
            "Epoch 289/500\n",
            "98/98 [==============================] - 28s 279ms/step - loss: 1.4092 - accuracy: 0.7579 - val_loss: 2.7970 - val_accuracy: 0.5341\n",
            "Epoch 290/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3946 - accuracy: 0.7622 - val_loss: 2.8716 - val_accuracy: 0.5318\n",
            "Epoch 291/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.3995 - accuracy: 0.7613 - val_loss: 2.7990 - val_accuracy: 0.5417\n",
            "Epoch 292/500\n",
            "98/98 [==============================] - 27s 279ms/step - loss: 1.3892 - accuracy: 0.7632 - val_loss: 2.7077 - val_accuracy: 0.5447\n",
            "Epoch 293/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.3988 - accuracy: 0.7609 - val_loss: 2.8249 - val_accuracy: 0.5397\n",
            "Epoch 294/500\n",
            "98/98 [==============================] - 27s 278ms/step - loss: 1.3899 - accuracy: 0.7605 - val_loss: 2.7629 - val_accuracy: 0.5387\n",
            "Epoch 295/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.3926 - accuracy: 0.7627 - val_loss: 2.9004 - val_accuracy: 0.5370\n",
            "Epoch 296/500\n",
            "98/98 [==============================] - 27s 279ms/step - loss: 1.3953 - accuracy: 0.7616 - val_loss: 2.8400 - val_accuracy: 0.5359\n",
            "Epoch 297/500\n",
            "98/98 [==============================] - 27s 278ms/step - loss: 1.3949 - accuracy: 0.7605 - val_loss: 2.8085 - val_accuracy: 0.5447\n",
            "Epoch 298/500\n",
            "98/98 [==============================] - 27s 279ms/step - loss: 1.4050 - accuracy: 0.7584 - val_loss: 2.7664 - val_accuracy: 0.5327\n",
            "Epoch 299/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3839 - accuracy: 0.7653 - val_loss: 2.8666 - val_accuracy: 0.5356\n",
            "Epoch 300/500\n",
            "98/98 [==============================] - 27s 279ms/step - loss: 1.4028 - accuracy: 0.7570 - val_loss: 2.8235 - val_accuracy: 0.5415\n",
            "Epoch 301/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3792 - accuracy: 0.7639 - val_loss: 2.7538 - val_accuracy: 0.5401\n",
            "Epoch 302/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.3889 - accuracy: 0.7630 - val_loss: 2.7579 - val_accuracy: 0.5463\n",
            "Epoch 303/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3876 - accuracy: 0.7638 - val_loss: 2.7348 - val_accuracy: 0.5441\n",
            "Epoch 304/500\n",
            "98/98 [==============================] - 27s 279ms/step - loss: 1.3882 - accuracy: 0.7625 - val_loss: 2.8458 - val_accuracy: 0.5297\n",
            "Epoch 305/500\n",
            "98/98 [==============================] - 28s 279ms/step - loss: 1.3788 - accuracy: 0.7664 - val_loss: 2.7955 - val_accuracy: 0.5436\n",
            "Epoch 306/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 1.3860 - accuracy: 0.7645 - val_loss: 2.9064 - val_accuracy: 0.5391\n",
            "Epoch 307/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.3896 - accuracy: 0.7629 - val_loss: 2.9420 - val_accuracy: 0.5303\n",
            "Epoch 308/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3906 - accuracy: 0.7626 - val_loss: 2.7400 - val_accuracy: 0.5353\n",
            "Epoch 309/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3750 - accuracy: 0.7680 - val_loss: 2.8803 - val_accuracy: 0.5400\n",
            "Epoch 310/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3810 - accuracy: 0.7663 - val_loss: 2.7548 - val_accuracy: 0.5415\n",
            "Epoch 311/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3816 - accuracy: 0.7660 - val_loss: 2.7391 - val_accuracy: 0.5393\n",
            "Epoch 312/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3803 - accuracy: 0.7673 - val_loss: 2.8343 - val_accuracy: 0.5406\n",
            "Epoch 313/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3812 - accuracy: 0.7642 - val_loss: 2.8084 - val_accuracy: 0.5404\n",
            "Epoch 314/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3838 - accuracy: 0.7643 - val_loss: 2.8504 - val_accuracy: 0.5381\n",
            "Epoch 315/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3801 - accuracy: 0.7657 - val_loss: 2.8017 - val_accuracy: 0.5428\n",
            "Epoch 316/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3730 - accuracy: 0.7670 - val_loss: 2.8549 - val_accuracy: 0.5282\n",
            "Epoch 317/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3901 - accuracy: 0.7619 - val_loss: 2.7639 - val_accuracy: 0.5394\n",
            "Epoch 318/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3712 - accuracy: 0.7678 - val_loss: 2.7625 - val_accuracy: 0.5386\n",
            "Epoch 319/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.3850 - accuracy: 0.7649 - val_loss: 2.7506 - val_accuracy: 0.5324\n",
            "Epoch 320/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3932 - accuracy: 0.7632 - val_loss: 2.7432 - val_accuracy: 0.5432\n",
            "Epoch 321/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3799 - accuracy: 0.7665 - val_loss: 2.8222 - val_accuracy: 0.5430\n",
            "Epoch 322/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3775 - accuracy: 0.7668 - val_loss: 2.8137 - val_accuracy: 0.5479\n",
            "Epoch 323/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.3737 - accuracy: 0.7673 - val_loss: 2.7120 - val_accuracy: 0.5430\n",
            "Epoch 324/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3774 - accuracy: 0.7681 - val_loss: 2.8086 - val_accuracy: 0.5397\n",
            "Epoch 325/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3842 - accuracy: 0.7652 - val_loss: 2.8130 - val_accuracy: 0.5380\n",
            "Epoch 326/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3694 - accuracy: 0.7706 - val_loss: 2.8624 - val_accuracy: 0.5263\n",
            "Epoch 327/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.3706 - accuracy: 0.7676 - val_loss: 2.8921 - val_accuracy: 0.5313\n",
            "Epoch 328/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3786 - accuracy: 0.7657 - val_loss: 2.8209 - val_accuracy: 0.5387\n",
            "Epoch 329/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3648 - accuracy: 0.7684 - val_loss: 2.8458 - val_accuracy: 0.5381\n",
            "Epoch 330/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3745 - accuracy: 0.7691 - val_loss: 2.7099 - val_accuracy: 0.5387\n",
            "Epoch 331/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3664 - accuracy: 0.7708 - val_loss: 2.7575 - val_accuracy: 0.5438\n",
            "Epoch 332/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3766 - accuracy: 0.7656 - val_loss: 2.7700 - val_accuracy: 0.5343\n",
            "Epoch 333/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3788 - accuracy: 0.7679 - val_loss: 2.7778 - val_accuracy: 0.5473\n",
            "Epoch 334/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3771 - accuracy: 0.7656 - val_loss: 2.8384 - val_accuracy: 0.5359\n",
            "Epoch 335/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 1.3631 - accuracy: 0.7707 - val_loss: 2.8388 - val_accuracy: 0.5361\n",
            "Epoch 336/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3699 - accuracy: 0.7692 - val_loss: 2.8801 - val_accuracy: 0.5335\n",
            "Epoch 337/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3696 - accuracy: 0.7690 - val_loss: 2.7914 - val_accuracy: 0.5396\n",
            "Epoch 338/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.3773 - accuracy: 0.7664 - val_loss: 2.8343 - val_accuracy: 0.5373\n",
            "Epoch 339/500\n",
            "98/98 [==============================] - 29s 289ms/step - loss: 1.3642 - accuracy: 0.7705 - val_loss: 2.9306 - val_accuracy: 0.5389\n",
            "Epoch 340/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3644 - accuracy: 0.7705 - val_loss: 2.9863 - val_accuracy: 0.5310\n",
            "Epoch 341/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3671 - accuracy: 0.7702 - val_loss: 2.7786 - val_accuracy: 0.5446\n",
            "Epoch 342/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3714 - accuracy: 0.7695 - val_loss: 2.7177 - val_accuracy: 0.5384\n",
            "Epoch 343/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3606 - accuracy: 0.7722 - val_loss: 2.8050 - val_accuracy: 0.5385\n",
            "Epoch 344/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.3664 - accuracy: 0.7700 - val_loss: 2.8355 - val_accuracy: 0.5415\n",
            "Epoch 345/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3702 - accuracy: 0.7694 - val_loss: 2.7929 - val_accuracy: 0.5391\n",
            "Epoch 346/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3786 - accuracy: 0.7678 - val_loss: 2.8811 - val_accuracy: 0.5383\n",
            "Epoch 347/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3554 - accuracy: 0.7719 - val_loss: 2.9488 - val_accuracy: 0.5338\n",
            "Epoch 348/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3732 - accuracy: 0.7690 - val_loss: 2.7982 - val_accuracy: 0.5353\n",
            "Epoch 349/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3604 - accuracy: 0.7733 - val_loss: 2.7544 - val_accuracy: 0.5459\n",
            "Epoch 350/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.3496 - accuracy: 0.7751 - val_loss: 2.8244 - val_accuracy: 0.5465\n",
            "Epoch 351/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3589 - accuracy: 0.7728 - val_loss: 2.8749 - val_accuracy: 0.5318\n",
            "Epoch 352/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 1.3672 - accuracy: 0.7707 - val_loss: 2.9617 - val_accuracy: 0.5346\n",
            "Epoch 353/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3692 - accuracy: 0.7689 - val_loss: 2.7691 - val_accuracy: 0.5371\n",
            "Epoch 354/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3700 - accuracy: 0.7694 - val_loss: 2.8587 - val_accuracy: 0.5351\n",
            "Epoch 355/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3598 - accuracy: 0.7725 - val_loss: 2.8821 - val_accuracy: 0.5392\n",
            "Epoch 356/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3625 - accuracy: 0.7727 - val_loss: 2.8371 - val_accuracy: 0.5392\n",
            "Epoch 357/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3640 - accuracy: 0.7722 - val_loss: 2.8987 - val_accuracy: 0.5394\n",
            "Epoch 358/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3499 - accuracy: 0.7742 - val_loss: 2.8282 - val_accuracy: 0.5387\n",
            "Epoch 359/500\n",
            "98/98 [==============================] - 30s 304ms/step - loss: 1.3607 - accuracy: 0.7729 - val_loss: 2.9558 - val_accuracy: 0.5317\n",
            "Epoch 360/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.3482 - accuracy: 0.7758 - val_loss: 2.9029 - val_accuracy: 0.5366\n",
            "Epoch 361/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.3525 - accuracy: 0.7757 - val_loss: 2.8284 - val_accuracy: 0.5403\n",
            "Epoch 362/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.3600 - accuracy: 0.7726 - val_loss: 2.8056 - val_accuracy: 0.5463\n",
            "Epoch 363/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3530 - accuracy: 0.7748 - val_loss: 2.8343 - val_accuracy: 0.5418\n",
            "Epoch 364/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3511 - accuracy: 0.7757 - val_loss: 2.9273 - val_accuracy: 0.5390\n",
            "Epoch 365/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3561 - accuracy: 0.7743 - val_loss: 2.7918 - val_accuracy: 0.5431\n",
            "Epoch 366/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3604 - accuracy: 0.7746 - val_loss: 2.8165 - val_accuracy: 0.5440\n",
            "Epoch 367/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3645 - accuracy: 0.7721 - val_loss: 2.8394 - val_accuracy: 0.5336\n",
            "Epoch 368/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3567 - accuracy: 0.7745 - val_loss: 2.8689 - val_accuracy: 0.5395\n",
            "Epoch 369/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3407 - accuracy: 0.7794 - val_loss: 2.9596 - val_accuracy: 0.5320\n",
            "Epoch 370/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3431 - accuracy: 0.7777 - val_loss: 2.8601 - val_accuracy: 0.5296\n",
            "Epoch 371/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3547 - accuracy: 0.7738 - val_loss: 2.9224 - val_accuracy: 0.5382\n",
            "Epoch 372/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3509 - accuracy: 0.7765 - val_loss: 2.8366 - val_accuracy: 0.5450\n",
            "Epoch 373/500\n",
            "98/98 [==============================] - 29s 289ms/step - loss: 1.3438 - accuracy: 0.7778 - val_loss: 2.8316 - val_accuracy: 0.5418\n",
            "Epoch 374/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.3593 - accuracy: 0.7728 - val_loss: 2.8513 - val_accuracy: 0.5415\n",
            "Epoch 375/500\n",
            "98/98 [==============================] - 27s 279ms/step - loss: 1.3526 - accuracy: 0.7746 - val_loss: 3.0333 - val_accuracy: 0.5393\n",
            "Epoch 376/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3655 - accuracy: 0.7710 - val_loss: 2.7609 - val_accuracy: 0.5351\n",
            "Epoch 377/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3563 - accuracy: 0.7739 - val_loss: 2.7842 - val_accuracy: 0.5416\n",
            "Epoch 378/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3550 - accuracy: 0.7748 - val_loss: 3.0270 - val_accuracy: 0.5363\n",
            "Epoch 379/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3519 - accuracy: 0.7758 - val_loss: 2.9072 - val_accuracy: 0.5401\n",
            "Epoch 380/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3375 - accuracy: 0.7790 - val_loss: 3.0503 - val_accuracy: 0.5289\n",
            "Epoch 381/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.3385 - accuracy: 0.7794 - val_loss: 2.8837 - val_accuracy: 0.5408\n",
            "Epoch 382/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3528 - accuracy: 0.7746 - val_loss: 2.7869 - val_accuracy: 0.5434\n",
            "Epoch 383/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3551 - accuracy: 0.7745 - val_loss: 2.8093 - val_accuracy: 0.5417\n",
            "Epoch 384/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3391 - accuracy: 0.7800 - val_loss: 2.8870 - val_accuracy: 0.5352\n",
            "Epoch 385/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3463 - accuracy: 0.7778 - val_loss: 2.8135 - val_accuracy: 0.5418\n",
            "Epoch 386/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3545 - accuracy: 0.7749 - val_loss: 2.8573 - val_accuracy: 0.5302\n",
            "Epoch 387/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3518 - accuracy: 0.7763 - val_loss: 2.9004 - val_accuracy: 0.5371\n",
            "Epoch 388/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3358 - accuracy: 0.7806 - val_loss: 2.9360 - val_accuracy: 0.5332\n",
            "Epoch 389/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.3435 - accuracy: 0.7766 - val_loss: 2.7925 - val_accuracy: 0.5438\n",
            "Epoch 390/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3329 - accuracy: 0.7815 - val_loss: 2.8257 - val_accuracy: 0.5343\n",
            "Epoch 391/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3446 - accuracy: 0.7779 - val_loss: 2.8167 - val_accuracy: 0.5462\n",
            "Epoch 392/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3373 - accuracy: 0.7794 - val_loss: 2.8115 - val_accuracy: 0.5398\n",
            "Epoch 393/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3399 - accuracy: 0.7786 - val_loss: 2.8384 - val_accuracy: 0.5369\n",
            "Epoch 394/500\n",
            "98/98 [==============================] - 28s 289ms/step - loss: 1.3491 - accuracy: 0.7761 - val_loss: 2.8510 - val_accuracy: 0.5403\n",
            "Epoch 395/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.3458 - accuracy: 0.7776 - val_loss: 2.7940 - val_accuracy: 0.5446\n",
            "Epoch 396/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3331 - accuracy: 0.7807 - val_loss: 2.8002 - val_accuracy: 0.5363\n",
            "Epoch 397/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3398 - accuracy: 0.7788 - val_loss: 2.9271 - val_accuracy: 0.5413\n",
            "Epoch 398/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.3391 - accuracy: 0.7794 - val_loss: 2.7729 - val_accuracy: 0.5356\n",
            "Epoch 399/500\n",
            "98/98 [==============================] - 28s 288ms/step - loss: 1.3376 - accuracy: 0.7797 - val_loss: 2.8483 - val_accuracy: 0.5391\n",
            "Epoch 400/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3267 - accuracy: 0.7840 - val_loss: 2.9052 - val_accuracy: 0.5342\n",
            "Epoch 401/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3244 - accuracy: 0.7835 - val_loss: 2.8317 - val_accuracy: 0.5363\n",
            "Epoch 402/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3282 - accuracy: 0.7829 - val_loss: 2.8476 - val_accuracy: 0.5477\n",
            "Epoch 403/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3389 - accuracy: 0.7793 - val_loss: 2.8124 - val_accuracy: 0.5386\n",
            "Epoch 404/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3402 - accuracy: 0.7793 - val_loss: 2.8736 - val_accuracy: 0.5353\n",
            "Epoch 405/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3453 - accuracy: 0.7771 - val_loss: 2.7452 - val_accuracy: 0.5382\n",
            "Epoch 406/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3331 - accuracy: 0.7826 - val_loss: 2.9214 - val_accuracy: 0.5365\n",
            "Epoch 407/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3412 - accuracy: 0.7794 - val_loss: 2.8550 - val_accuracy: 0.5409\n",
            "Epoch 408/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3254 - accuracy: 0.7849 - val_loss: 2.8837 - val_accuracy: 0.5369\n",
            "Epoch 409/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3436 - accuracy: 0.7784 - val_loss: 2.8413 - val_accuracy: 0.5374\n",
            "Epoch 410/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 1.3441 - accuracy: 0.7779 - val_loss: 2.8870 - val_accuracy: 0.5326\n",
            "Epoch 411/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3440 - accuracy: 0.7785 - val_loss: 2.8723 - val_accuracy: 0.5351\n",
            "Epoch 412/500\n",
            "98/98 [==============================] - 29s 300ms/step - loss: 1.3349 - accuracy: 0.7818 - val_loss: 2.7881 - val_accuracy: 0.5381\n",
            "Epoch 413/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.3272 - accuracy: 0.7829 - val_loss: 2.7924 - val_accuracy: 0.5400\n",
            "Epoch 414/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3268 - accuracy: 0.7826 - val_loss: 2.8546 - val_accuracy: 0.5326\n",
            "Epoch 415/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3341 - accuracy: 0.7804 - val_loss: 2.7934 - val_accuracy: 0.5349\n",
            "Epoch 416/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3376 - accuracy: 0.7802 - val_loss: 2.8480 - val_accuracy: 0.5392\n",
            "Epoch 417/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3341 - accuracy: 0.7818 - val_loss: 2.8347 - val_accuracy: 0.5327\n",
            "Epoch 418/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3288 - accuracy: 0.7831 - val_loss: 2.7845 - val_accuracy: 0.5462\n",
            "Epoch 419/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3387 - accuracy: 0.7792 - val_loss: 2.9219 - val_accuracy: 0.5334\n",
            "Epoch 420/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3250 - accuracy: 0.7834 - val_loss: 2.8105 - val_accuracy: 0.5462\n",
            "Epoch 421/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3246 - accuracy: 0.7841 - val_loss: 2.8237 - val_accuracy: 0.5371\n",
            "Epoch 422/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3246 - accuracy: 0.7845 - val_loss: 2.8581 - val_accuracy: 0.5407\n",
            "Epoch 423/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3255 - accuracy: 0.7842 - val_loss: 2.8372 - val_accuracy: 0.5412\n",
            "Epoch 424/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3338 - accuracy: 0.7838 - val_loss: 2.9367 - val_accuracy: 0.5339\n",
            "Epoch 425/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3208 - accuracy: 0.7851 - val_loss: 2.8769 - val_accuracy: 0.5355\n",
            "Epoch 426/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3364 - accuracy: 0.7789 - val_loss: 2.8678 - val_accuracy: 0.5400\n",
            "Epoch 427/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3328 - accuracy: 0.7805 - val_loss: 2.9375 - val_accuracy: 0.5380\n",
            "Epoch 428/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3456 - accuracy: 0.7786 - val_loss: 2.8701 - val_accuracy: 0.5375\n",
            "Epoch 429/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3363 - accuracy: 0.7793 - val_loss: 2.8458 - val_accuracy: 0.5332\n",
            "Epoch 430/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3270 - accuracy: 0.7840 - val_loss: 2.8639 - val_accuracy: 0.5320\n",
            "Epoch 431/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3192 - accuracy: 0.7843 - val_loss: 2.9265 - val_accuracy: 0.5383\n",
            "Epoch 432/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.3229 - accuracy: 0.7833 - val_loss: 2.9408 - val_accuracy: 0.5440\n",
            "Epoch 433/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3208 - accuracy: 0.7854 - val_loss: 2.8315 - val_accuracy: 0.5318\n",
            "Epoch 434/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3216 - accuracy: 0.7856 - val_loss: 2.9861 - val_accuracy: 0.5299\n",
            "Epoch 435/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3240 - accuracy: 0.7844 - val_loss: 2.9084 - val_accuracy: 0.5393\n",
            "Epoch 436/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3239 - accuracy: 0.7853 - val_loss: 2.9728 - val_accuracy: 0.5353\n",
            "Epoch 437/500\n",
            "98/98 [==============================] - 30s 308ms/step - loss: 1.3276 - accuracy: 0.7837 - val_loss: 2.8682 - val_accuracy: 0.5433\n",
            "Epoch 438/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3146 - accuracy: 0.7870 - val_loss: 3.0045 - val_accuracy: 0.5314\n",
            "Epoch 439/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3277 - accuracy: 0.7836 - val_loss: 2.8647 - val_accuracy: 0.5388\n",
            "Epoch 440/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3430 - accuracy: 0.7795 - val_loss: 2.7580 - val_accuracy: 0.5441\n",
            "Epoch 441/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3172 - accuracy: 0.7858 - val_loss: 2.8997 - val_accuracy: 0.5382\n",
            "Epoch 442/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3213 - accuracy: 0.7847 - val_loss: 2.9585 - val_accuracy: 0.5421\n",
            "Epoch 443/500\n",
            "98/98 [==============================] - 29s 295ms/step - loss: 1.3223 - accuracy: 0.7844 - val_loss: 2.9029 - val_accuracy: 0.5396\n",
            "Epoch 444/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3307 - accuracy: 0.7831 - val_loss: 2.9816 - val_accuracy: 0.5350\n",
            "Epoch 445/500\n",
            "98/98 [==============================] - 29s 298ms/step - loss: 1.3304 - accuracy: 0.7833 - val_loss: 2.8714 - val_accuracy: 0.5343\n",
            "Epoch 446/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.3185 - accuracy: 0.7857 - val_loss: 2.9695 - val_accuracy: 0.5398\n",
            "Epoch 447/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3224 - accuracy: 0.7862 - val_loss: 2.8631 - val_accuracy: 0.5373\n",
            "Epoch 448/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3228 - accuracy: 0.7850 - val_loss: 2.7843 - val_accuracy: 0.5351\n",
            "Epoch 449/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3164 - accuracy: 0.7880 - val_loss: 2.9268 - val_accuracy: 0.5398\n",
            "Epoch 450/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3223 - accuracy: 0.7861 - val_loss: 2.9390 - val_accuracy: 0.5325\n",
            "Epoch 451/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3210 - accuracy: 0.7852 - val_loss: 2.8835 - val_accuracy: 0.5448\n",
            "Epoch 452/500\n",
            "98/98 [==============================] - 28s 287ms/step - loss: 1.3288 - accuracy: 0.7839 - val_loss: 2.8960 - val_accuracy: 0.5388\n",
            "Epoch 453/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3188 - accuracy: 0.7861 - val_loss: 2.8873 - val_accuracy: 0.5354\n",
            "Epoch 454/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3234 - accuracy: 0.7845 - val_loss: 2.9112 - val_accuracy: 0.5351\n",
            "Epoch 455/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.3188 - accuracy: 0.7869 - val_loss: 2.8564 - val_accuracy: 0.5412\n",
            "Epoch 456/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3246 - accuracy: 0.7862 - val_loss: 2.9602 - val_accuracy: 0.5307\n",
            "Epoch 457/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3151 - accuracy: 0.7868 - val_loss: 2.8525 - val_accuracy: 0.5420\n",
            "Epoch 458/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3119 - accuracy: 0.7876 - val_loss: 2.9162 - val_accuracy: 0.5502\n",
            "Epoch 459/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.3163 - accuracy: 0.7872 - val_loss: 2.9040 - val_accuracy: 0.5324\n",
            "Epoch 460/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3196 - accuracy: 0.7858 - val_loss: 2.9277 - val_accuracy: 0.5337\n",
            "Epoch 461/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3165 - accuracy: 0.7872 - val_loss: 2.9292 - val_accuracy: 0.5402\n",
            "Epoch 462/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3223 - accuracy: 0.7840 - val_loss: 2.9144 - val_accuracy: 0.5380\n",
            "Epoch 463/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3138 - accuracy: 0.7887 - val_loss: 2.9469 - val_accuracy: 0.5413\n",
            "Epoch 464/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3196 - accuracy: 0.7868 - val_loss: 2.8939 - val_accuracy: 0.5383\n",
            "Epoch 465/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3147 - accuracy: 0.7867 - val_loss: 2.8035 - val_accuracy: 0.5356\n",
            "Epoch 466/500\n",
            "98/98 [==============================] - 29s 290ms/step - loss: 1.3065 - accuracy: 0.7886 - val_loss: 2.9440 - val_accuracy: 0.5385\n",
            "Epoch 467/500\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 1.3141 - accuracy: 0.7873 - val_loss: 2.8656 - val_accuracy: 0.5340\n",
            "Epoch 468/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3232 - accuracy: 0.7843 - val_loss: 3.0261 - val_accuracy: 0.5316\n",
            "Epoch 469/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3138 - accuracy: 0.7884 - val_loss: 2.9485 - val_accuracy: 0.5470\n",
            "Epoch 470/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3153 - accuracy: 0.7858 - val_loss: 2.8979 - val_accuracy: 0.5359\n",
            "Epoch 471/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3138 - accuracy: 0.7869 - val_loss: 2.9725 - val_accuracy: 0.5308\n",
            "Epoch 472/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3236 - accuracy: 0.7849 - val_loss: 2.9528 - val_accuracy: 0.5465\n",
            "Epoch 473/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3089 - accuracy: 0.7910 - val_loss: 2.9892 - val_accuracy: 0.5389\n",
            "Epoch 474/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3095 - accuracy: 0.7905 - val_loss: 2.9007 - val_accuracy: 0.5412\n",
            "Epoch 475/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3199 - accuracy: 0.7860 - val_loss: 2.9337 - val_accuracy: 0.5401\n",
            "Epoch 476/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3067 - accuracy: 0.7903 - val_loss: 2.8998 - val_accuracy: 0.5342\n",
            "Epoch 477/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3086 - accuracy: 0.7889 - val_loss: 3.0397 - val_accuracy: 0.5404\n",
            "Epoch 478/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.2958 - accuracy: 0.7922 - val_loss: 2.8900 - val_accuracy: 0.5389\n",
            "Epoch 479/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.3082 - accuracy: 0.7892 - val_loss: 2.9291 - val_accuracy: 0.5438\n",
            "Epoch 480/500\n",
            "98/98 [==============================] - 28s 284ms/step - loss: 1.3167 - accuracy: 0.7873 - val_loss: 2.9189 - val_accuracy: 0.5347\n",
            "Epoch 481/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3153 - accuracy: 0.7876 - val_loss: 2.9316 - val_accuracy: 0.5294\n",
            "Epoch 482/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3216 - accuracy: 0.7853 - val_loss: 2.9464 - val_accuracy: 0.5413\n",
            "Epoch 483/500\n",
            "98/98 [==============================] - 28s 286ms/step - loss: 1.3270 - accuracy: 0.7841 - val_loss: 2.8927 - val_accuracy: 0.5379\n",
            "Epoch 484/500\n",
            "98/98 [==============================] - 30s 300ms/step - loss: 1.3115 - accuracy: 0.7874 - val_loss: 2.9045 - val_accuracy: 0.5337\n",
            "Epoch 485/500\n",
            "98/98 [==============================] - 29s 293ms/step - loss: 1.3066 - accuracy: 0.7895 - val_loss: 3.0856 - val_accuracy: 0.5413\n",
            "Epoch 486/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3203 - accuracy: 0.7853 - val_loss: 2.9075 - val_accuracy: 0.5424\n",
            "Epoch 487/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3129 - accuracy: 0.7886 - val_loss: 2.7989 - val_accuracy: 0.5370\n",
            "Epoch 488/500\n",
            "98/98 [==============================] - 28s 283ms/step - loss: 1.3018 - accuracy: 0.7922 - val_loss: 3.0338 - val_accuracy: 0.5292\n",
            "Epoch 489/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3221 - accuracy: 0.7854 - val_loss: 2.9212 - val_accuracy: 0.5391\n",
            "Epoch 490/500\n",
            "98/98 [==============================] - 29s 294ms/step - loss: 1.3114 - accuracy: 0.7879 - val_loss: 2.8830 - val_accuracy: 0.5496\n",
            "Epoch 491/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3005 - accuracy: 0.7925 - val_loss: 3.0201 - val_accuracy: 0.5441\n",
            "Epoch 492/500\n",
            "98/98 [==============================] - 28s 285ms/step - loss: 1.3017 - accuracy: 0.7923 - val_loss: 3.0566 - val_accuracy: 0.5356\n",
            "Epoch 493/500\n",
            "98/98 [==============================] - 29s 291ms/step - loss: 1.3188 - accuracy: 0.7870 - val_loss: 3.0424 - val_accuracy: 0.5334\n",
            "Epoch 494/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3128 - accuracy: 0.7889 - val_loss: 2.9197 - val_accuracy: 0.5338\n",
            "Epoch 495/500\n",
            "98/98 [==============================] - 27s 280ms/step - loss: 1.2955 - accuracy: 0.7935 - val_loss: 2.9051 - val_accuracy: 0.5429\n",
            "Epoch 496/500\n",
            "98/98 [==============================] - 28s 282ms/step - loss: 1.3003 - accuracy: 0.7912 - val_loss: 2.9178 - val_accuracy: 0.5442\n",
            "Epoch 497/500\n",
            "98/98 [==============================] - 30s 303ms/step - loss: 1.3161 - accuracy: 0.7884 - val_loss: 2.9265 - val_accuracy: 0.5406\n",
            "Epoch 498/500\n",
            "98/98 [==============================] - 28s 290ms/step - loss: 1.2967 - accuracy: 0.7922 - val_loss: 3.0234 - val_accuracy: 0.5367\n",
            "Epoch 499/500\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 1.3024 - accuracy: 0.7904 - val_loss: 3.0857 - val_accuracy: 0.5298\n",
            "Epoch 500/500\n",
            "98/98 [==============================] - 29s 292ms/step - loss: 1.3057 - accuracy: 0.7908 - val_loss: 2.9214 - val_accuracy: 0.5390\n"
          ]
        }
      ],
      "source": [
        "model = build_model()\n",
        "print(model.summary())\n",
        "import os\n",
        "# set callback\n",
        "save_dir = '/content/drive/MyDrive/ECE6930/NiN-0207-02/new1'\n",
        "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "checkpointer = keras.callbacks.ModelCheckpoint(os.path.join(save_dir, '{epoch:03d}.h5'), monitor='val_loss', verbose=0,\t\t\t\t\t\tsave_best_only=False, \n",
        " \t\t\t\t\t\t\t\tsave_weights_only=False, mode='auto', \n",
        " \t\t\t\t\t\t\t\tperiod=10)\n",
        "\n",
        "\n",
        "\n",
        "# set data augmentation\n",
        "print('Using real-time data augmentation.')\n",
        "datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# start training\n",
        "history = model.fit(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                    #steps_per_epoch=iterations,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[checkpointer],\n",
        "                    validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eaU6yE9WqLtp"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "model.save('nin.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SEx0D5S-eLlz",
        "outputId": "b9838343-693c-42d1-f633-fa08a7806aae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\",\"test\"],loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}